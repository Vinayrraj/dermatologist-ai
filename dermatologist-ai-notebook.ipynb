{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "def print_elapsed_time():\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "def start_timer():\n",
    "    elapsed = (timeit.default_timer() - start_time)/60\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinayrraj\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3811088810837933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files   \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(data_path, shuffle=None):\n",
    "    kwargs = {}\n",
    "    if shuffle != None:\n",
    "        kwargs['shuffle'] = shuffle\n",
    "    data = load_files(data_path, **kwargs)\n",
    "    img_files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), 3)\n",
    "    return img_files, targets\n",
    "\n",
    "\n",
    "\n",
    "start_timer()\n",
    "train_files, train_targets = load_dataset('data/train')\n",
    "valid_files, valid_targets = load_dataset('data/valid')\n",
    "test_files, test_targets = load_dataset('data/test', shuffle=False)\n",
    "\n",
    "# load lables\n",
    "label_name = [item[11:-1] for item in sorted(glob(\"data/train/*/\"))]\n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.190188388953173\n",
      "train_files size: 2000\n",
      "train_files shape: (2000,)\n",
      "target shape: (2000, 3)\n",
      "['melanoma', 'nevus', 'seborrheic_keratosis']\n",
      "7.765273693394192e-07\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "print('train_files size: {}'.format(len(train_files)))\n",
    "print('train_files shape: {}'.format(train_files.shape))\n",
    "print('target shape: {}'.format(train_targets.shape))\n",
    "print(label_name)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "elapsed = (timeit.default_timer() - start_time)/60\n",
    "print(elapsed)\n",
    "\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003094760232234724\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size=(384, 256))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(image_paths):\n",
    "    return np.vstack([path_to_tensor(path) for path in image_paths])\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning and putting images into tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0024611650982516646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:17<00:00,  5.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:52<00:00,  2.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [05:25<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 384, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "train_tensors = paths_to_tensor(tqdm(train_files))\n",
    "valid_tensors = paths_to_tensor(tqdm(valid_files))\n",
    "test_tensors = paths_to_tensor(tqdm(test_files))\n",
    "\n",
    "print(train_tensors.shape)\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flip Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.656683204139657\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "apply_train_image_transform = False\n",
    "\n",
    "if apply_train_image_transform:\n",
    "    # Caution: Doesn't guarantee prevention of duplication.\n",
    "    datagen_train = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "    \n",
    "    datagen_train.fit(train_tensors)\n",
    "    shape = (train_tensors.shape[0] * 2,) + train_tensors.shape[1:]\n",
    "    generated = np.ndarray(shape=shape)\n",
    "    for i, image in tqdm(enumerate(train_tensors)):\n",
    "        generated[i] = datagen_train.random_transform(image)\n",
    "    \n",
    "    train_tensors = np.concatenate((train_tensors, generated))\n",
    "    train_targets = train_targets.repeat(2, axis=0)\n",
    "    \n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning using Inception Resnet V2¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.681337163063922\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "train_imgs_preprocess = preprocess_input(train_tensors)\n",
    "valid_imgs_preprocess = preprocess_input(valid_tensors)\n",
    "test_imgs_preprocess = preprocess_input(test_tensors)\n",
    "del train_tensors, valid_tensors, test_tensors\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.714697982052833\n",
      "(2000, 10, 6, 1536)\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "transfer_model = InceptionResNetV2(include_top=False)\n",
    "\n",
    "train_data = transfer_model.predict(train_imgs_preprocess)\n",
    "valid_data = transfer_model.predict(valid_imgs_preprocess)\n",
    "test_data = transfer_model.predict(test_imgs_preprocess)\n",
    "\n",
    "del train_imgs_preprocess, valid_imgs_preprocess, test_imgs_preprocess\n",
    "print(train_data.shape)\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.14202191513837\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 1,576,963\n",
      "Trainable params: 1,576,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "from keras.layers import Conv2D, Dropout, Flatten, Dense, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "my_model = Sequential()\n",
    "\n",
    "my_model.add(GlobalAveragePooling2D(input_shape=train_data.shape[1:]))\n",
    "my_model.add(Dropout(0.2))\n",
    "my_model.add(Dense(1024, activation='relu'))\n",
    "my_model.add(Dropout(0.2))\n",
    "my_model.add(Dense(3, activation='softmax'))\n",
    "my_model.summary()\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.14523349580675\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.14736855354285\n",
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/70\n",
      "2000/2000 [==============================] - ETA: 29s - loss: 1.0736 - acc: 0.48 - ETA: 13s - loss: 2.4214 - acc: 0.58 - ETA: 8s - loss: 2.2929 - acc: 0.6283 - ETA: 5s - loss: 2.9955 - acc: 0.530 - ETA: 3s - loss: 2.9371 - acc: 0.517 - ETA: 2s - loss: 2.9326 - acc: 0.549 - ETA: 1s - loss: 3.0722 - acc: 0.565 - ETA: 1s - loss: 3.1481 - acc: 0.583 - ETA: 0s - loss: 3.2026 - acc: 0.595 - 5s 3ms/step - loss: 3.2636 - acc: 0.6035 - val_loss: 5.6274 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.62745, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 2/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 3.3109 - acc: 0.685 - ETA: 1s - loss: 2.9709 - acc: 0.697 - ETA: 0s - loss: 2.7767 - acc: 0.681 - ETA: 0s - loss: 2.7757 - acc: 0.605 - ETA: 0s - loss: 2.7267 - acc: 0.558 - ETA: 0s - loss: 2.6732 - acc: 0.570 - ETA: 0s - loss: 2.6565 - acc: 0.578 - ETA: 0s - loss: 2.6858 - acc: 0.591 - ETA: 0s - loss: 2.7033 - acc: 0.601 - 1s 719us/step - loss: 2.6794 - acc: 0.6070 - val_loss: 4.8214 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.62745 to 4.82141, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 3/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 2.1337 - acc: 0.720 - ETA: 1s - loss: 2.0760 - acc: 0.702 - ETA: 0s - loss: 1.9837 - acc: 0.708 - ETA: 0s - loss: 2.0087 - acc: 0.676 - ETA: 0s - loss: 1.9011 - acc: 0.659 - ETA: 0s - loss: 1.7762 - acc: 0.650 - ETA: 0s - loss: 1.6992 - acc: 0.647 - ETA: 0s - loss: 1.5827 - acc: 0.657 - ETA: 0s - loss: 1.5047 - acc: 0.657 - 1s 719us/step - loss: 1.4369 - acc: 0.6595 - val_loss: 1.0059 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.82141 to 1.00587, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 4/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.7663 - acc: 0.710 - ETA: 1s - loss: 0.8192 - acc: 0.702 - ETA: 0s - loss: 0.8498 - acc: 0.696 - ETA: 0s - loss: 0.8627 - acc: 0.687 - ETA: 0s - loss: 0.8752 - acc: 0.681 - ETA: 0s - loss: 0.8653 - acc: 0.686 - ETA: 0s - loss: 0.8744 - acc: 0.682 - ETA: 0s - loss: 0.8720 - acc: 0.683 - ETA: 0s - loss: 0.8666 - acc: 0.682 - 1s 714us/step - loss: 0.8635 - acc: 0.6795 - val_loss: 0.9564 - val_acc: 0.5267\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00587 to 0.95643, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 5/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.7522 - acc: 0.680 - ETA: 1s - loss: 0.7535 - acc: 0.685 - ETA: 0s - loss: 0.7215 - acc: 0.713 - ETA: 0s - loss: 0.7390 - acc: 0.703 - ETA: 0s - loss: 0.7290 - acc: 0.705 - ETA: 0s - loss: 0.7182 - acc: 0.712 - ETA: 0s - loss: 0.7485 - acc: 0.698 - ETA: 0s - loss: 0.7625 - acc: 0.695 - ETA: 0s - loss: 0.7723 - acc: 0.691 - 1s 719us/step - loss: 0.7687 - acc: 0.6930 - val_loss: 1.0074 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.95643\n",
      "Epoch 6/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.7440 - acc: 0.715 - ETA: 1s - loss: 0.7771 - acc: 0.680 - ETA: 0s - loss: 0.7573 - acc: 0.696 - ETA: 0s - loss: 0.7308 - acc: 0.710 - ETA: 0s - loss: 0.7241 - acc: 0.716 - ETA: 0s - loss: 0.7104 - acc: 0.722 - ETA: 0s - loss: 0.7099 - acc: 0.717 - ETA: 0s - loss: 0.7182 - acc: 0.710 - ETA: 0s - loss: 0.7239 - acc: 0.707 - 1s 716us/step - loss: 0.7370 - acc: 0.6970 - val_loss: 0.9017 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.95643 to 0.90168, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 7/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6522 - acc: 0.740 - ETA: 1s - loss: 0.6658 - acc: 0.732 - ETA: 0s - loss: 0.6705 - acc: 0.730 - ETA: 0s - loss: 0.6695 - acc: 0.722 - ETA: 0s - loss: 0.6811 - acc: 0.714 - ETA: 0s - loss: 0.6964 - acc: 0.701 - ETA: 0s - loss: 0.7065 - acc: 0.695 - ETA: 0s - loss: 0.7131 - acc: 0.690 - ETA: 0s - loss: 0.7120 - acc: 0.692 - 1s 719us/step - loss: 0.7087 - acc: 0.6945 - val_loss: 0.8792 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.90168 to 0.87918, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 8/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.7345 - acc: 0.675 - ETA: 1s - loss: 0.7261 - acc: 0.695 - ETA: 0s - loss: 0.7210 - acc: 0.705 - ETA: 0s - loss: 0.7061 - acc: 0.715 - ETA: 0s - loss: 0.6828 - acc: 0.727 - ETA: 0s - loss: 0.6857 - acc: 0.721 - ETA: 0s - loss: 0.6843 - acc: 0.717 - ETA: 0s - loss: 0.6775 - acc: 0.716 - ETA: 0s - loss: 0.6869 - acc: 0.709 - 1s 727us/step - loss: 0.6847 - acc: 0.7105 - val_loss: 0.8522 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.87918 to 0.85217, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 9/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.7353 - acc: 0.685 - ETA: 1s - loss: 0.6740 - acc: 0.717 - ETA: 0s - loss: 0.6713 - acc: 0.706 - ETA: 0s - loss: 0.6746 - acc: 0.706 - ETA: 0s - loss: 0.6762 - acc: 0.708 - ETA: 0s - loss: 0.6663 - acc: 0.713 - ETA: 0s - loss: 0.6705 - acc: 0.710 - ETA: 0s - loss: 0.6665 - acc: 0.716 - ETA: 0s - loss: 0.6634 - acc: 0.718 - 1s 719us/step - loss: 0.6611 - acc: 0.7210 - val_loss: 0.8107 - val_acc: 0.6267\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.85217 to 0.81071, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 10/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6346 - acc: 0.755 - ETA: 1s - loss: 0.6834 - acc: 0.725 - ETA: 0s - loss: 0.6641 - acc: 0.721 - ETA: 0s - loss: 0.6468 - acc: 0.726 - ETA: 0s - loss: 0.6331 - acc: 0.732 - ETA: 0s - loss: 0.6439 - acc: 0.727 - ETA: 0s - loss: 0.6369 - acc: 0.730 - ETA: 0s - loss: 0.6317 - acc: 0.730 - ETA: 0s - loss: 0.6334 - acc: 0.731 - 1s 730us/step - loss: 0.6333 - acc: 0.7310 - val_loss: 0.7867 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.81071 to 0.78672, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 11/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6336 - acc: 0.740 - ETA: 1s - loss: 0.6484 - acc: 0.730 - ETA: 0s - loss: 0.6605 - acc: 0.723 - ETA: 0s - loss: 0.6534 - acc: 0.727 - ETA: 0s - loss: 0.6510 - acc: 0.731 - ETA: 0s - loss: 0.6399 - acc: 0.736 - ETA: 0s - loss: 0.6405 - acc: 0.739 - ETA: 0s - loss: 0.6338 - acc: 0.739 - ETA: 0s - loss: 0.6390 - acc: 0.731 - 2s 818us/step - loss: 0.6373 - acc: 0.7325 - val_loss: 0.7996 - val_acc: 0.6467\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.78672\n",
      "Epoch 12/70\n",
      "2000/2000 [==============================] - ETA: 2s - loss: 0.6172 - acc: 0.730 - ETA: 1s - loss: 0.6483 - acc: 0.705 - ETA: 1s - loss: 0.6393 - acc: 0.710 - ETA: 1s - loss: 0.6274 - acc: 0.727 - ETA: 1s - loss: 0.6291 - acc: 0.726 - ETA: 0s - loss: 0.6277 - acc: 0.727 - ETA: 0s - loss: 0.6284 - acc: 0.727 - ETA: 0s - loss: 0.6276 - acc: 0.729 - ETA: 0s - loss: 0.6254 - acc: 0.733 - 2s 931us/step - loss: 0.6188 - acc: 0.7370 - val_loss: 0.8049 - val_acc: 0.6400\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.78672\n",
      "Epoch 13/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6324 - acc: 0.705 - ETA: 1s - loss: 0.6011 - acc: 0.732 - ETA: 0s - loss: 0.6146 - acc: 0.735 - ETA: 0s - loss: 0.6253 - acc: 0.725 - ETA: 0s - loss: 0.6152 - acc: 0.731 - ETA: 0s - loss: 0.6137 - acc: 0.728 - ETA: 0s - loss: 0.6084 - acc: 0.732 - ETA: 0s - loss: 0.6075 - acc: 0.734 - ETA: 0s - loss: 0.6010 - acc: 0.739 - 1s 722us/step - loss: 0.6054 - acc: 0.7360 - val_loss: 0.8138 - val_acc: 0.6467\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.78672\n",
      "Epoch 14/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5227 - acc: 0.770 - ETA: 1s - loss: 0.6224 - acc: 0.720 - ETA: 0s - loss: 0.6085 - acc: 0.728 - ETA: 0s - loss: 0.6159 - acc: 0.731 - ETA: 0s - loss: 0.6186 - acc: 0.731 - ETA: 0s - loss: 0.6066 - acc: 0.739 - ETA: 0s - loss: 0.6117 - acc: 0.735 - ETA: 0s - loss: 0.6089 - acc: 0.730 - ETA: 0s - loss: 0.6011 - acc: 0.735 - 1s 726us/step - loss: 0.6053 - acc: 0.7330 - val_loss: 0.7893 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.78672\n",
      "Epoch 15/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5771 - acc: 0.750 - ETA: 1s - loss: 0.5981 - acc: 0.750 - ETA: 0s - loss: 0.6088 - acc: 0.740 - ETA: 0s - loss: 0.5935 - acc: 0.742 - ETA: 0s - loss: 0.5919 - acc: 0.745 - ETA: 0s - loss: 0.5796 - acc: 0.751 - ETA: 0s - loss: 0.5898 - acc: 0.747 - ETA: 0s - loss: 0.5876 - acc: 0.749 - ETA: 0s - loss: 0.5879 - acc: 0.748 - 1s 716us/step - loss: 0.5845 - acc: 0.7485 - val_loss: 0.7530 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.78672 to 0.75301, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 16/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5846 - acc: 0.755 - ETA: 1s - loss: 0.5938 - acc: 0.740 - ETA: 0s - loss: 0.5792 - acc: 0.760 - ETA: 0s - loss: 0.5782 - acc: 0.757 - ETA: 0s - loss: 0.5670 - acc: 0.766 - ETA: 0s - loss: 0.5708 - acc: 0.765 - ETA: 0s - loss: 0.5746 - acc: 0.763 - ETA: 0s - loss: 0.5796 - acc: 0.765 - ETA: 0s - loss: 0.5797 - acc: 0.767 - 1s 726us/step - loss: 0.5818 - acc: 0.7650 - val_loss: 0.7467 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.75301 to 0.74670, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 17/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6505 - acc: 0.725 - ETA: 1s - loss: 0.6143 - acc: 0.732 - ETA: 0s - loss: 0.5941 - acc: 0.750 - ETA: 0s - loss: 0.5974 - acc: 0.741 - ETA: 0s - loss: 0.5897 - acc: 0.746 - ETA: 0s - loss: 0.5801 - acc: 0.755 - ETA: 0s - loss: 0.5701 - acc: 0.762 - ETA: 0s - loss: 0.5698 - acc: 0.761 - ETA: 0s - loss: 0.5681 - acc: 0.760 - 1s 725us/step - loss: 0.5705 - acc: 0.7575 - val_loss: 0.7323 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.74670 to 0.73232, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 18/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5432 - acc: 0.770 - ETA: 1s - loss: 0.6037 - acc: 0.730 - ETA: 0s - loss: 0.5961 - acc: 0.741 - ETA: 0s - loss: 0.5751 - acc: 0.755 - ETA: 0s - loss: 0.5693 - acc: 0.761 - ETA: 0s - loss: 0.5810 - acc: 0.756 - ETA: 0s - loss: 0.5871 - acc: 0.755 - ETA: 0s - loss: 0.5871 - acc: 0.755 - ETA: 0s - loss: 0.5786 - acc: 0.758 - 1s 716us/step - loss: 0.5786 - acc: 0.7570 - val_loss: 0.7282 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.73232 to 0.72821, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 19/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6018 - acc: 0.760 - ETA: 1s - loss: 0.5669 - acc: 0.772 - ETA: 0s - loss: 0.5550 - acc: 0.783 - ETA: 0s - loss: 0.5763 - acc: 0.773 - ETA: 0s - loss: 0.5733 - acc: 0.770 - ETA: 0s - loss: 0.5787 - acc: 0.765 - ETA: 0s - loss: 0.5736 - acc: 0.762 - ETA: 0s - loss: 0.5709 - acc: 0.763 - ETA: 0s - loss: 0.5717 - acc: 0.759 - 1s 719us/step - loss: 0.5673 - acc: 0.7605 - val_loss: 0.7402 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.72821\n",
      "Epoch 20/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6245 - acc: 0.720 - ETA: 1s - loss: 0.5771 - acc: 0.752 - ETA: 0s - loss: 0.5666 - acc: 0.756 - ETA: 0s - loss: 0.5514 - acc: 0.767 - ETA: 0s - loss: 0.5555 - acc: 0.767 - ETA: 0s - loss: 0.5568 - acc: 0.765 - ETA: 0s - loss: 0.5578 - acc: 0.762 - ETA: 0s - loss: 0.5523 - acc: 0.766 - ETA: 0s - loss: 0.5496 - acc: 0.768 - 1s 719us/step - loss: 0.5511 - acc: 0.7675 - val_loss: 0.7554 - val_acc: 0.6733\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.72821\n",
      "Epoch 21/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5047 - acc: 0.785 - ETA: 1s - loss: 0.5278 - acc: 0.780 - ETA: 0s - loss: 0.5392 - acc: 0.771 - ETA: 0s - loss: 0.5394 - acc: 0.775 - ETA: 0s - loss: 0.5418 - acc: 0.779 - ETA: 0s - loss: 0.5399 - acc: 0.781 - ETA: 0s - loss: 0.5392 - acc: 0.784 - ETA: 0s - loss: 0.5562 - acc: 0.769 - ETA: 0s - loss: 0.5519 - acc: 0.768 - 1s 724us/step - loss: 0.5509 - acc: 0.7705 - val_loss: 0.7831 - val_acc: 0.6533\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.72821\n",
      "Epoch 22/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5859 - acc: 0.740 - ETA: 1s - loss: 0.5677 - acc: 0.747 - ETA: 0s - loss: 0.5521 - acc: 0.761 - ETA: 0s - loss: 0.5426 - acc: 0.770 - ETA: 0s - loss: 0.5346 - acc: 0.769 - ETA: 0s - loss: 0.5426 - acc: 0.762 - ETA: 0s - loss: 0.5519 - acc: 0.753 - ETA: 0s - loss: 0.5501 - acc: 0.755 - ETA: 0s - loss: 0.5518 - acc: 0.757 - 1s 719us/step - loss: 0.5507 - acc: 0.7610 - val_loss: 0.7457 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.72821\n",
      "Epoch 23/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5744 - acc: 0.750 - ETA: 1s - loss: 0.5375 - acc: 0.767 - ETA: 0s - loss: 0.4994 - acc: 0.791 - ETA: 0s - loss: 0.5178 - acc: 0.786 - ETA: 0s - loss: 0.5126 - acc: 0.790 - ETA: 0s - loss: 0.5184 - acc: 0.785 - ETA: 0s - loss: 0.5356 - acc: 0.777 - ETA: 0s - loss: 0.5415 - acc: 0.779 - ETA: 0s - loss: 0.5383 - acc: 0.778 - 1s 743us/step - loss: 0.5371 - acc: 0.7785 - val_loss: 0.7272 - val_acc: 0.6733\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.72821 to 0.72723, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 24/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5235 - acc: 0.800 - ETA: 1s - loss: 0.5113 - acc: 0.790 - ETA: 0s - loss: 0.5179 - acc: 0.781 - ETA: 0s - loss: 0.5234 - acc: 0.776 - ETA: 0s - loss: 0.5211 - acc: 0.776 - ETA: 0s - loss: 0.5267 - acc: 0.775 - ETA: 0s - loss: 0.5305 - acc: 0.772 - ETA: 0s - loss: 0.5266 - acc: 0.776 - ETA: 0s - loss: 0.5261 - acc: 0.776 - 1s 727us/step - loss: 0.5307 - acc: 0.7760 - val_loss: 0.7915 - val_acc: 0.6533\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.72723\n",
      "Epoch 25/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5715 - acc: 0.765 - ETA: 1s - loss: 0.5478 - acc: 0.747 - ETA: 0s - loss: 0.5355 - acc: 0.755 - ETA: 0s - loss: 0.5382 - acc: 0.760 - ETA: 0s - loss: 0.5456 - acc: 0.758 - ETA: 0s - loss: 0.5462 - acc: 0.761 - ETA: 0s - loss: 0.5289 - acc: 0.777 - ETA: 0s - loss: 0.5291 - acc: 0.773 - ETA: 0s - loss: 0.5261 - acc: 0.775 - 1s 719us/step - loss: 0.5277 - acc: 0.7765 - val_loss: 0.7362 - val_acc: 0.6733\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.72723\n",
      "Epoch 26/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4761 - acc: 0.810 - ETA: 1s - loss: 0.5432 - acc: 0.782 - ETA: 0s - loss: 0.5237 - acc: 0.800 - ETA: 0s - loss: 0.5162 - acc: 0.798 - ETA: 0s - loss: 0.5135 - acc: 0.797 - ETA: 0s - loss: 0.5138 - acc: 0.792 - ETA: 0s - loss: 0.5149 - acc: 0.792 - ETA: 0s - loss: 0.5200 - acc: 0.790 - ETA: 0s - loss: 0.5294 - acc: 0.784 - 1s 721us/step - loss: 0.5332 - acc: 0.7775 - val_loss: 0.6853 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.72723 to 0.68532, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 27/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5410 - acc: 0.770 - ETA: 1s - loss: 0.5250 - acc: 0.785 - ETA: 0s - loss: 0.5569 - acc: 0.766 - ETA: 0s - loss: 0.5546 - acc: 0.773 - ETA: 0s - loss: 0.5326 - acc: 0.779 - ETA: 0s - loss: 0.5279 - acc: 0.782 - ETA: 0s - loss: 0.5235 - acc: 0.785 - ETA: 0s - loss: 0.5230 - acc: 0.786 - ETA: 0s - loss: 0.5309 - acc: 0.783 - 1s 727us/step - loss: 0.5284 - acc: 0.7860 - val_loss: 0.7110 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.68532\n",
      "Epoch 28/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4170 - acc: 0.875 - ETA: 1s - loss: 0.4731 - acc: 0.822 - ETA: 0s - loss: 0.4796 - acc: 0.811 - ETA: 0s - loss: 0.5062 - acc: 0.793 - ETA: 0s - loss: 0.5302 - acc: 0.778 - ETA: 0s - loss: 0.5218 - acc: 0.785 - ETA: 0s - loss: 0.5244 - acc: 0.785 - ETA: 0s - loss: 0.5255 - acc: 0.780 - ETA: 0s - loss: 0.5181 - acc: 0.781 - 1s 721us/step - loss: 0.5158 - acc: 0.7830 - val_loss: 0.8739 - val_acc: 0.6200\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.68532\n",
      "Epoch 29/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6370 - acc: 0.695 - ETA: 1s - loss: 0.5903 - acc: 0.727 - ETA: 0s - loss: 0.5555 - acc: 0.760 - ETA: 0s - loss: 0.5406 - acc: 0.770 - ETA: 0s - loss: 0.5272 - acc: 0.774 - ETA: 0s - loss: 0.5300 - acc: 0.772 - ETA: 0s - loss: 0.5237 - acc: 0.775 - ETA: 0s - loss: 0.5208 - acc: 0.780 - ETA: 0s - loss: 0.5222 - acc: 0.776 - 1s 728us/step - loss: 0.5222 - acc: 0.7795 - val_loss: 0.6901 - val_acc: 0.6933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: val_loss did not improve from 0.68532\n",
      "Epoch 30/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4574 - acc: 0.820 - ETA: 1s - loss: 0.4677 - acc: 0.825 - ETA: 0s - loss: 0.5081 - acc: 0.803 - ETA: 0s - loss: 0.4987 - acc: 0.800 - ETA: 0s - loss: 0.5130 - acc: 0.791 - ETA: 0s - loss: 0.5048 - acc: 0.793 - ETA: 0s - loss: 0.5169 - acc: 0.788 - ETA: 0s - loss: 0.5117 - acc: 0.789 - ETA: 0s - loss: 0.5071 - acc: 0.792 - 1s 725us/step - loss: 0.5060 - acc: 0.7905 - val_loss: 0.7048 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.68532\n",
      "Epoch 31/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5407 - acc: 0.800 - ETA: 1s - loss: 0.5220 - acc: 0.787 - ETA: 0s - loss: 0.5078 - acc: 0.783 - ETA: 0s - loss: 0.5041 - acc: 0.785 - ETA: 0s - loss: 0.4959 - acc: 0.793 - ETA: 0s - loss: 0.5086 - acc: 0.785 - ETA: 0s - loss: 0.4915 - acc: 0.796 - ETA: 0s - loss: 0.4920 - acc: 0.792 - ETA: 0s - loss: 0.4904 - acc: 0.795 - 1s 724us/step - loss: 0.4853 - acc: 0.7975 - val_loss: 0.7124 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.68532\n",
      "Epoch 32/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5171 - acc: 0.755 - ETA: 1s - loss: 0.5195 - acc: 0.772 - ETA: 0s - loss: 0.5085 - acc: 0.785 - ETA: 0s - loss: 0.5071 - acc: 0.790 - ETA: 0s - loss: 0.4864 - acc: 0.806 - ETA: 0s - loss: 0.4825 - acc: 0.804 - ETA: 0s - loss: 0.4745 - acc: 0.810 - ETA: 0s - loss: 0.4785 - acc: 0.805 - ETA: 0s - loss: 0.4853 - acc: 0.797 - 1s 722us/step - loss: 0.4871 - acc: 0.7990 - val_loss: 0.7536 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.68532\n",
      "Epoch 33/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6382 - acc: 0.725 - ETA: 1s - loss: 0.5498 - acc: 0.775 - ETA: 0s - loss: 0.5338 - acc: 0.776 - ETA: 0s - loss: 0.5239 - acc: 0.780 - ETA: 0s - loss: 0.5232 - acc: 0.783 - ETA: 0s - loss: 0.5195 - acc: 0.784 - ETA: 0s - loss: 0.5109 - acc: 0.787 - ETA: 0s - loss: 0.4926 - acc: 0.797 - ETA: 0s - loss: 0.4921 - acc: 0.795 - 1s 723us/step - loss: 0.4918 - acc: 0.7960 - val_loss: 0.7131 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.68532\n",
      "Epoch 34/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5021 - acc: 0.775 - ETA: 1s - loss: 0.4657 - acc: 0.820 - ETA: 0s - loss: 0.4757 - acc: 0.805 - ETA: 0s - loss: 0.4782 - acc: 0.807 - ETA: 0s - loss: 0.4662 - acc: 0.810 - ETA: 0s - loss: 0.4640 - acc: 0.808 - ETA: 0s - loss: 0.4801 - acc: 0.802 - ETA: 0s - loss: 0.4942 - acc: 0.798 - ETA: 0s - loss: 0.4912 - acc: 0.800 - 1s 722us/step - loss: 0.4936 - acc: 0.8010 - val_loss: 0.7037 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.68532\n",
      "Epoch 35/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4930 - acc: 0.780 - ETA: 1s - loss: 0.4929 - acc: 0.790 - ETA: 0s - loss: 0.4958 - acc: 0.793 - ETA: 0s - loss: 0.5034 - acc: 0.787 - ETA: 0s - loss: 0.5003 - acc: 0.781 - ETA: 0s - loss: 0.4967 - acc: 0.781 - ETA: 0s - loss: 0.4997 - acc: 0.783 - ETA: 0s - loss: 0.4979 - acc: 0.791 - ETA: 0s - loss: 0.4959 - acc: 0.793 - 1s 719us/step - loss: 0.4895 - acc: 0.7980 - val_loss: 0.7541 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.68532\n",
      "Epoch 36/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5158 - acc: 0.805 - ETA: 1s - loss: 0.5489 - acc: 0.777 - ETA: 0s - loss: 0.5327 - acc: 0.780 - ETA: 0s - loss: 0.5169 - acc: 0.791 - ETA: 0s - loss: 0.5068 - acc: 0.802 - ETA: 0s - loss: 0.5054 - acc: 0.802 - ETA: 0s - loss: 0.4933 - acc: 0.810 - ETA: 0s - loss: 0.5015 - acc: 0.806 - ETA: 0s - loss: 0.5010 - acc: 0.803 - 1s 722us/step - loss: 0.4914 - acc: 0.8080 - val_loss: 0.7084 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.68532\n",
      "Epoch 37/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4831 - acc: 0.775 - ETA: 1s - loss: 0.4935 - acc: 0.787 - ETA: 0s - loss: 0.5194 - acc: 0.780 - ETA: 0s - loss: 0.5105 - acc: 0.775 - ETA: 0s - loss: 0.4917 - acc: 0.786 - ETA: 0s - loss: 0.4877 - acc: 0.786 - ETA: 0s - loss: 0.5080 - acc: 0.781 - ETA: 0s - loss: 0.4989 - acc: 0.787 - ETA: 0s - loss: 0.5003 - acc: 0.787 - 1s 719us/step - loss: 0.4984 - acc: 0.7865 - val_loss: 0.6863 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.68532\n",
      "Epoch 38/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4676 - acc: 0.815 - ETA: 1s - loss: 0.4607 - acc: 0.810 - ETA: 0s - loss: 0.4648 - acc: 0.808 - ETA: 0s - loss: 0.4699 - acc: 0.811 - ETA: 0s - loss: 0.4691 - acc: 0.811 - ETA: 0s - loss: 0.4715 - acc: 0.810 - ETA: 0s - loss: 0.4634 - acc: 0.813 - ETA: 0s - loss: 0.4572 - acc: 0.815 - ETA: 0s - loss: 0.4620 - acc: 0.808 - 1s 724us/step - loss: 0.4639 - acc: 0.8085 - val_loss: 0.7034 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.68532\n",
      "Epoch 39/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4865 - acc: 0.805 - ETA: 1s - loss: 0.4502 - acc: 0.842 - ETA: 0s - loss: 0.4545 - acc: 0.835 - ETA: 0s - loss: 0.4478 - acc: 0.827 - ETA: 0s - loss: 0.4483 - acc: 0.824 - ETA: 0s - loss: 0.4530 - acc: 0.820 - ETA: 0s - loss: 0.4539 - acc: 0.817 - ETA: 0s - loss: 0.4634 - acc: 0.813 - ETA: 0s - loss: 0.4565 - acc: 0.818 - 1s 711us/step - loss: 0.4558 - acc: 0.8185 - val_loss: 0.7608 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.68532\n",
      "Epoch 40/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4462 - acc: 0.820 - ETA: 1s - loss: 0.4443 - acc: 0.820 - ETA: 0s - loss: 0.4606 - acc: 0.801 - ETA: 0s - loss: 0.4621 - acc: 0.796 - ETA: 0s - loss: 0.4580 - acc: 0.801 - ETA: 0s - loss: 0.4516 - acc: 0.800 - ETA: 0s - loss: 0.4538 - acc: 0.800 - ETA: 0s - loss: 0.4556 - acc: 0.804 - ETA: 0s - loss: 0.4534 - acc: 0.806 - 1s 716us/step - loss: 0.4556 - acc: 0.8055 - val_loss: 0.7526 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.68532\n",
      "Epoch 41/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4512 - acc: 0.820 - ETA: 1s - loss: 0.4432 - acc: 0.817 - ETA: 0s - loss: 0.4563 - acc: 0.806 - ETA: 0s - loss: 0.4479 - acc: 0.811 - ETA: 0s - loss: 0.4562 - acc: 0.812 - ETA: 0s - loss: 0.4556 - acc: 0.807 - ETA: 0s - loss: 0.4552 - acc: 0.810 - ETA: 0s - loss: 0.4517 - acc: 0.812 - ETA: 0s - loss: 0.4468 - acc: 0.815 - 1s 714us/step - loss: 0.4513 - acc: 0.8140 - val_loss: 0.7401 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.68532\n",
      "Epoch 42/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4431 - acc: 0.835 - ETA: 1s - loss: 0.4456 - acc: 0.812 - ETA: 0s - loss: 0.4403 - acc: 0.815 - ETA: 0s - loss: 0.4443 - acc: 0.818 - ETA: 0s - loss: 0.4377 - acc: 0.817 - ETA: 0s - loss: 0.4514 - acc: 0.808 - ETA: 0s - loss: 0.4453 - acc: 0.816 - ETA: 0s - loss: 0.4410 - acc: 0.823 - ETA: 0s - loss: 0.4440 - acc: 0.821 - 1s 719us/step - loss: 0.4446 - acc: 0.8225 - val_loss: 0.6748 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.68532 to 0.67479, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 43/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5004 - acc: 0.795 - ETA: 1s - loss: 0.4907 - acc: 0.787 - ETA: 0s - loss: 0.4586 - acc: 0.798 - ETA: 0s - loss: 0.4553 - acc: 0.802 - ETA: 0s - loss: 0.4434 - acc: 0.804 - ETA: 0s - loss: 0.4463 - acc: 0.805 - ETA: 0s - loss: 0.4484 - acc: 0.806 - ETA: 0s - loss: 0.4522 - acc: 0.806 - ETA: 0s - loss: 0.4492 - acc: 0.810 - 1s 727us/step - loss: 0.4454 - acc: 0.8135 - val_loss: 0.7913 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.67479\n",
      "Epoch 44/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4070 - acc: 0.825 - ETA: 1s - loss: 0.4006 - acc: 0.822 - ETA: 0s - loss: 0.4323 - acc: 0.810 - ETA: 0s - loss: 0.4304 - acc: 0.813 - ETA: 0s - loss: 0.4354 - acc: 0.814 - ETA: 0s - loss: 0.4425 - acc: 0.810 - ETA: 0s - loss: 0.4426 - acc: 0.811 - ETA: 0s - loss: 0.4413 - acc: 0.810 - ETA: 0s - loss: 0.4397 - acc: 0.813 - 1s 714us/step - loss: 0.4477 - acc: 0.8110 - val_loss: 0.7802 - val_acc: 0.6733\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.67479\n",
      "Epoch 45/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3691 - acc: 0.845 - ETA: 1s - loss: 0.4107 - acc: 0.827 - ETA: 0s - loss: 0.4171 - acc: 0.830 - ETA: 0s - loss: 0.4253 - acc: 0.821 - ETA: 0s - loss: 0.4225 - acc: 0.825 - ETA: 0s - loss: 0.4181 - acc: 0.830 - ETA: 0s - loss: 0.4194 - acc: 0.829 - ETA: 0s - loss: 0.4217 - acc: 0.830 - ETA: 0s - loss: 0.4276 - acc: 0.826 - 1s 724us/step - loss: 0.4277 - acc: 0.8280 - val_loss: 0.7046 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.67479\n",
      "Epoch 46/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4223 - acc: 0.840 - ETA: 1s - loss: 0.4613 - acc: 0.810 - ETA: 0s - loss: 0.4563 - acc: 0.815 - ETA: 0s - loss: 0.4437 - acc: 0.818 - ETA: 0s - loss: 0.4520 - acc: 0.814 - ETA: 0s - loss: 0.4521 - acc: 0.815 - ETA: 0s - loss: 0.4480 - acc: 0.820 - ETA: 0s - loss: 0.4411 - acc: 0.825 - ETA: 0s - loss: 0.4386 - acc: 0.828 - 1s 728us/step - loss: 0.4354 - acc: 0.8285 - val_loss: 0.7702 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.67479\n",
      "Epoch 47/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4408 - acc: 0.810 - ETA: 1s - loss: 0.4524 - acc: 0.800 - ETA: 0s - loss: 0.4352 - acc: 0.818 - ETA: 0s - loss: 0.4199 - acc: 0.825 - ETA: 0s - loss: 0.4170 - acc: 0.825 - ETA: 0s - loss: 0.4225 - acc: 0.818 - ETA: 0s - loss: 0.4313 - acc: 0.813 - ETA: 0s - loss: 0.4354 - acc: 0.813 - ETA: 0s - loss: 0.4298 - acc: 0.815 - 1s 721us/step - loss: 0.4307 - acc: 0.8175 - val_loss: 0.7674 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.67479\n",
      "Epoch 48/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4649 - acc: 0.800 - ETA: 1s - loss: 0.4361 - acc: 0.820 - ETA: 0s - loss: 0.4066 - acc: 0.830 - ETA: 0s - loss: 0.4291 - acc: 0.822 - ETA: 0s - loss: 0.4422 - acc: 0.818 - ETA: 0s - loss: 0.4305 - acc: 0.827 - ETA: 0s - loss: 0.4337 - acc: 0.821 - ETA: 0s - loss: 0.4429 - acc: 0.815 - ETA: 0s - loss: 0.4420 - acc: 0.817 - 1s 723us/step - loss: 0.4400 - acc: 0.8180 - val_loss: 0.7549 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.67479\n",
      "Epoch 49/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4719 - acc: 0.765 - ETA: 1s - loss: 0.4334 - acc: 0.792 - ETA: 0s - loss: 0.4102 - acc: 0.816 - ETA: 0s - loss: 0.4070 - acc: 0.818 - ETA: 0s - loss: 0.4207 - acc: 0.813 - ETA: 0s - loss: 0.4139 - acc: 0.819 - ETA: 0s - loss: 0.4171 - acc: 0.818 - ETA: 0s - loss: 0.4172 - acc: 0.821 - ETA: 0s - loss: 0.4224 - acc: 0.818 - 1s 724us/step - loss: 0.4235 - acc: 0.8185 - val_loss: 0.7354 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.67479\n",
      "Epoch 50/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3328 - acc: 0.880 - ETA: 1s - loss: 0.3726 - acc: 0.852 - ETA: 0s - loss: 0.3777 - acc: 0.856 - ETA: 0s - loss: 0.3820 - acc: 0.856 - ETA: 0s - loss: 0.3908 - acc: 0.850 - ETA: 0s - loss: 0.3788 - acc: 0.857 - ETA: 0s - loss: 0.3868 - acc: 0.850 - ETA: 0s - loss: 0.3856 - acc: 0.848 - ETA: 0s - loss: 0.3867 - acc: 0.847 - 1s 729us/step - loss: 0.3959 - acc: 0.8385 - val_loss: 0.7592 - val_acc: 0.6600\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.67479\n",
      "Epoch 51/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4698 - acc: 0.815 - ETA: 1s - loss: 0.4540 - acc: 0.815 - ETA: 0s - loss: 0.4404 - acc: 0.820 - ETA: 0s - loss: 0.4319 - acc: 0.828 - ETA: 0s - loss: 0.4326 - acc: 0.829 - ETA: 0s - loss: 0.4358 - acc: 0.824 - ETA: 0s - loss: 0.4256 - acc: 0.826 - ETA: 0s - loss: 0.4206 - acc: 0.828 - ETA: 0s - loss: 0.4211 - acc: 0.830 - 1s 729us/step - loss: 0.4183 - acc: 0.8305 - val_loss: 0.7344 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.67479\n",
      "Epoch 52/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4633 - acc: 0.815 - ETA: 1s - loss: 0.4246 - acc: 0.835 - ETA: 0s - loss: 0.4231 - acc: 0.830 - ETA: 0s - loss: 0.4036 - acc: 0.840 - ETA: 0s - loss: 0.4037 - acc: 0.839 - ETA: 0s - loss: 0.4037 - acc: 0.838 - ETA: 0s - loss: 0.4020 - acc: 0.841 - ETA: 0s - loss: 0.4083 - acc: 0.834 - ETA: 0s - loss: 0.4099 - acc: 0.835 - 1s 714us/step - loss: 0.4066 - acc: 0.8350 - val_loss: 0.7065 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.67479\n",
      "Epoch 53/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3833 - acc: 0.825 - ETA: 1s - loss: 0.4169 - acc: 0.822 - ETA: 0s - loss: 0.3964 - acc: 0.843 - ETA: 0s - loss: 0.4025 - acc: 0.836 - ETA: 0s - loss: 0.4039 - acc: 0.835 - ETA: 0s - loss: 0.4048 - acc: 0.835 - ETA: 0s - loss: 0.4093 - acc: 0.836 - ETA: 0s - loss: 0.4064 - acc: 0.838 - ETA: 0s - loss: 0.4029 - acc: 0.838 - 1s 709us/step - loss: 0.4002 - acc: 0.8385 - val_loss: 0.7866 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.67479\n",
      "Epoch 54/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3735 - acc: 0.845 - ETA: 1s - loss: 0.3558 - acc: 0.862 - ETA: 0s - loss: 0.3560 - acc: 0.856 - ETA: 0s - loss: 0.3675 - acc: 0.850 - ETA: 0s - loss: 0.3782 - acc: 0.848 - ETA: 0s - loss: 0.3817 - acc: 0.849 - ETA: 0s - loss: 0.3829 - acc: 0.849 - ETA: 0s - loss: 0.3887 - acc: 0.845 - ETA: 0s - loss: 0.3873 - acc: 0.842 - 1s 720us/step - loss: 0.3914 - acc: 0.8380 - val_loss: 0.7556 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.67479\n",
      "Epoch 55/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3765 - acc: 0.860 - ETA: 1s - loss: 0.3904 - acc: 0.847 - ETA: 0s - loss: 0.4010 - acc: 0.838 - ETA: 0s - loss: 0.3909 - acc: 0.852 - ETA: 0s - loss: 0.3875 - acc: 0.852 - ETA: 0s - loss: 0.3908 - acc: 0.851 - ETA: 0s - loss: 0.3867 - acc: 0.852 - ETA: 0s - loss: 0.3886 - acc: 0.849 - ETA: 0s - loss: 0.3916 - acc: 0.846 - 1s 724us/step - loss: 0.3854 - acc: 0.8505 - val_loss: 0.7281 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.67479\n",
      "Epoch 56/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3586 - acc: 0.860 - ETA: 1s - loss: 0.3447 - acc: 0.865 - ETA: 0s - loss: 0.3449 - acc: 0.871 - ETA: 0s - loss: 0.3485 - acc: 0.861 - ETA: 0s - loss: 0.3456 - acc: 0.865 - ETA: 0s - loss: 0.3671 - acc: 0.850 - ETA: 0s - loss: 0.3719 - acc: 0.850 - ETA: 0s - loss: 0.3740 - acc: 0.848 - ETA: 0s - loss: 0.3810 - acc: 0.843 - 1s 716us/step - loss: 0.3781 - acc: 0.8445 - val_loss: 0.7318 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.67479\n",
      "Epoch 57/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3706 - acc: 0.830 - ETA: 1s - loss: 0.3843 - acc: 0.820 - ETA: 0s - loss: 0.3530 - acc: 0.848 - ETA: 0s - loss: 0.3546 - acc: 0.851 - ETA: 0s - loss: 0.3528 - acc: 0.856 - ETA: 0s - loss: 0.3642 - acc: 0.852 - ETA: 0s - loss: 0.3647 - acc: 0.852 - ETA: 0s - loss: 0.3759 - acc: 0.843 - ETA: 0s - loss: 0.3804 - acc: 0.839 - 1s 722us/step - loss: 0.3777 - acc: 0.8440 - val_loss: 0.7158 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.67479\n",
      "Epoch 58/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3136 - acc: 0.890 - ETA: 1s - loss: 0.3361 - acc: 0.860 - ETA: 0s - loss: 0.3419 - acc: 0.865 - ETA: 0s - loss: 0.3493 - acc: 0.858 - ETA: 0s - loss: 0.3563 - acc: 0.858 - ETA: 0s - loss: 0.3604 - acc: 0.857 - ETA: 0s - loss: 0.3680 - acc: 0.852 - ETA: 0s - loss: 0.3671 - acc: 0.852 - ETA: 0s - loss: 0.3637 - acc: 0.851 - 1s 718us/step - loss: 0.3666 - acc: 0.8520 - val_loss: 0.7703 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.67479\n",
      "Epoch 59/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3728 - acc: 0.865 - ETA: 1s - loss: 0.3768 - acc: 0.865 - ETA: 0s - loss: 0.3745 - acc: 0.858 - ETA: 0s - loss: 0.3746 - acc: 0.866 - ETA: 0s - loss: 0.3780 - acc: 0.860 - ETA: 0s - loss: 0.3807 - acc: 0.856 - ETA: 0s - loss: 0.3757 - acc: 0.855 - ETA: 0s - loss: 0.3711 - acc: 0.856 - ETA: 0s - loss: 0.3732 - acc: 0.853 - 1s 709us/step - loss: 0.3702 - acc: 0.8520 - val_loss: 0.7704 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.67479\n",
      "Epoch 60/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3538 - acc: 0.855 - ETA: 1s - loss: 0.3658 - acc: 0.852 - ETA: 0s - loss: 0.3665 - acc: 0.858 - ETA: 0s - loss: 0.3756 - acc: 0.851 - ETA: 0s - loss: 0.3807 - acc: 0.843 - ETA: 0s - loss: 0.3794 - acc: 0.840 - ETA: 0s - loss: 0.3763 - acc: 0.843 - ETA: 0s - loss: 0.3728 - acc: 0.846 - ETA: 0s - loss: 0.3662 - acc: 0.850 - 1s 719us/step - loss: 0.3701 - acc: 0.8480 - val_loss: 0.7948 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.67479\n",
      "Epoch 61/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3597 - acc: 0.850 - ETA: 1s - loss: 0.3443 - acc: 0.875 - ETA: 0s - loss: 0.3580 - acc: 0.868 - ETA: 0s - loss: 0.3734 - acc: 0.861 - ETA: 0s - loss: 0.3763 - acc: 0.859 - ETA: 0s - loss: 0.3622 - acc: 0.861 - ETA: 0s - loss: 0.3598 - acc: 0.857 - ETA: 0s - loss: 0.3501 - acc: 0.858 - ETA: 0s - loss: 0.3548 - acc: 0.855 - 1s 715us/step - loss: 0.3547 - acc: 0.8555 - val_loss: 0.7081 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.67479\n",
      "Epoch 62/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3590 - acc: 0.860 - ETA: 1s - loss: 0.3349 - acc: 0.867 - ETA: 0s - loss: 0.3368 - acc: 0.868 - ETA: 0s - loss: 0.3505 - acc: 0.857 - ETA: 0s - loss: 0.3534 - acc: 0.860 - ETA: 0s - loss: 0.3450 - acc: 0.864 - ETA: 0s - loss: 0.3486 - acc: 0.857 - ETA: 0s - loss: 0.3568 - acc: 0.856 - ETA: 0s - loss: 0.3626 - acc: 0.853 - 1s 711us/step - loss: 0.3558 - acc: 0.8575 - val_loss: 0.7841 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.67479\n",
      "Epoch 63/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3098 - acc: 0.885 - ETA: 1s - loss: 0.3234 - acc: 0.882 - ETA: 0s - loss: 0.3121 - acc: 0.881 - ETA: 0s - loss: 0.3335 - acc: 0.870 - ETA: 0s - loss: 0.3363 - acc: 0.873 - ETA: 0s - loss: 0.3418 - acc: 0.867 - ETA: 0s - loss: 0.3457 - acc: 0.865 - ETA: 0s - loss: 0.3504 - acc: 0.862 - ETA: 0s - loss: 0.3504 - acc: 0.861 - 1s 709us/step - loss: 0.3542 - acc: 0.8575 - val_loss: 0.6695 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.67479 to 0.66946, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 64/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.2757 - acc: 0.930 - ETA: 1s - loss: 0.3103 - acc: 0.895 - ETA: 0s - loss: 0.3352 - acc: 0.880 - ETA: 0s - loss: 0.3456 - acc: 0.872 - ETA: 0s - loss: 0.3522 - acc: 0.866 - ETA: 0s - loss: 0.3529 - acc: 0.865 - ETA: 0s - loss: 0.3495 - acc: 0.867 - ETA: 0s - loss: 0.3506 - acc: 0.867 - ETA: 0s - loss: 0.3489 - acc: 0.868 - 1s 719us/step - loss: 0.3490 - acc: 0.8650 - val_loss: 0.7428 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.66946\n",
      "Epoch 65/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3447 - acc: 0.875 - ETA: 1s - loss: 0.3548 - acc: 0.875 - ETA: 0s - loss: 0.3433 - acc: 0.873 - ETA: 0s - loss: 0.3363 - acc: 0.871 - ETA: 0s - loss: 0.3343 - acc: 0.868 - ETA: 0s - loss: 0.3345 - acc: 0.869 - ETA: 0s - loss: 0.3332 - acc: 0.868 - ETA: 0s - loss: 0.3298 - acc: 0.868 - ETA: 0s - loss: 0.3377 - acc: 0.864 - 1s 708us/step - loss: 0.3400 - acc: 0.8635 - val_loss: 0.8049 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.66946\n",
      "Epoch 66/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3642 - acc: 0.860 - ETA: 1s - loss: 0.3602 - acc: 0.845 - ETA: 0s - loss: 0.3560 - acc: 0.858 - ETA: 0s - loss: 0.3628 - acc: 0.858 - ETA: 0s - loss: 0.3529 - acc: 0.863 - ETA: 0s - loss: 0.3424 - acc: 0.866 - ETA: 0s - loss: 0.3515 - acc: 0.863 - ETA: 0s - loss: 0.3492 - acc: 0.860 - ETA: 0s - loss: 0.3479 - acc: 0.860 - 1s 709us/step - loss: 0.3424 - acc: 0.8655 - val_loss: 0.6950 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.66946\n",
      "Epoch 67/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3823 - acc: 0.840 - ETA: 1s - loss: 0.3541 - acc: 0.860 - ETA: 0s - loss: 0.3377 - acc: 0.865 - ETA: 0s - loss: 0.3416 - acc: 0.865 - ETA: 0s - loss: 0.3448 - acc: 0.864 - ETA: 0s - loss: 0.3422 - acc: 0.869 - ETA: 0s - loss: 0.3320 - acc: 0.873 - ETA: 0s - loss: 0.3367 - acc: 0.873 - ETA: 0s - loss: 0.3358 - acc: 0.872 - 1s 715us/step - loss: 0.3330 - acc: 0.8755 - val_loss: 0.6854 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.66946\n",
      "Epoch 68/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.2742 - acc: 0.910 - ETA: 1s - loss: 0.2938 - acc: 0.897 - ETA: 0s - loss: 0.2890 - acc: 0.901 - ETA: 0s - loss: 0.2833 - acc: 0.905 - ETA: 0s - loss: 0.2885 - acc: 0.904 - ETA: 0s - loss: 0.3061 - acc: 0.892 - ETA: 0s - loss: 0.3193 - acc: 0.884 - ETA: 0s - loss: 0.3221 - acc: 0.883 - ETA: 0s - loss: 0.3201 - acc: 0.883 - 1s 720us/step - loss: 0.3165 - acc: 0.8845 - val_loss: 0.8094 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.66946\n",
      "Epoch 69/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4145 - acc: 0.840 - ETA: 1s - loss: 0.3525 - acc: 0.855 - ETA: 0s - loss: 0.3461 - acc: 0.861 - ETA: 0s - loss: 0.3266 - acc: 0.875 - ETA: 0s - loss: 0.3291 - acc: 0.874 - ETA: 0s - loss: 0.3249 - acc: 0.871 - ETA: 0s - loss: 0.3235 - acc: 0.871 - ETA: 0s - loss: 0.3270 - acc: 0.870 - ETA: 0s - loss: 0.3304 - acc: 0.864 - 1s 718us/step - loss: 0.3307 - acc: 0.8645 - val_loss: 0.7370 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.66946\n",
      "Epoch 70/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.2937 - acc: 0.895 - ETA: 1s - loss: 0.3082 - acc: 0.870 - ETA: 0s - loss: 0.3080 - acc: 0.868 - ETA: 0s - loss: 0.3041 - acc: 0.871 - ETA: 0s - loss: 0.3139 - acc: 0.870 - ETA: 0s - loss: 0.3099 - acc: 0.872 - ETA: 0s - loss: 0.3056 - acc: 0.875 - ETA: 0s - loss: 0.3016 - acc: 0.881 - ETA: 0s - loss: 0.3089 - acc: 0.876 - 1s 713us/step - loss: 0.3102 - acc: 0.8765 - val_loss: 0.7684 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.66946\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "checkpoint_filepath = 'saved_models/weights.best.my.hdf5'\n",
    "\n",
    "my_checkpointer = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "my_model.fit(train_data, train_targets, \n",
    "          validation_data=(valid_data, valid_targets),\n",
    "          epochs=70, batch_size=200, callbacks=[my_checkpointer], verbose=1)\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.97895079205792\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "my_model.load_weights(checkpoint_filepath)\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluavate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.99812342347877\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "my_predictions = [my_model.predict(np.expand_dims(feature, axis=0)) for feature in test_data]\n",
    "\n",
    "# test_accuracy = 100 * np.sum(np.array(my_predictions)==np.argmax(test_targets, axis=1)) / len(my_predictions)\n",
    "# print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "\n",
    "with open('my_transfer.csv', 'w', newline='') as csvfile:\n",
    "    result_writger = csv.writer(csvfile)\n",
    "    result_writger.writerow(['Id', 'task_1', 'task_2'])\n",
    "    for test_filepath, test_prediction in zip(test_files, my_predictions):\n",
    "        result_writger.writerow([test_filepath, test_prediction[0][0], test_prediction[0][2]])\n",
    "        \n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category 1 Score: 0.526\n",
    "Category 2 Score: 0.606\n",
    "Category 3 Score: 0.566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ROC Curves](images/figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without transfer learning, loading images into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.04965888261978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:33<00:00,  5.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:54<00:00,  2.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [05:33<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 384, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "train_tensors = paths_to_tensor(tqdm(train_files))\n",
    "train_tensors = train_tensors.astype('float32') / 255\n",
    "\n",
    "valid_tensors = paths_to_tensor(tqdm(valid_files))\n",
    "valid_tensors = valid_tensors.astype('float32') / 255\n",
    "\n",
    "test_tensors = paths_to_tensor(tqdm(test_files))\n",
    "test_tensors = test_tensors.astype('float32') / 255\n",
    "\n",
    "print(train_tensors.shape)\n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.24814145427362\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_204 (Conv2D)          (None, 384, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 192, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 192, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_205 (Conv2D)          (None, 192, 128, 64)      9280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 96, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 96, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_206 (Conv2D)          (None, 96, 64, 256)       147712    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 48, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 48, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_207 (Conv2D)          (None, 48, 32, 1024)      2360320   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 24, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 24, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_208 (Conv2D)          (None, 24, 16, 2048)      18876416  \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 12, 8, 2048)       0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12, 8, 2048)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 21,400,323\n",
      "Trainable params: 21,400,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "from keras.layers import Conv2D, Dropout, Flatten, Dense, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "my_model = Sequential()\n",
    "\n",
    "my_model.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', \n",
    "                        input_shape=train_tensors.shape[1:]))\n",
    "my_model.add(MaxPooling2D(pool_size=2))\n",
    "my_model.add(Dropout(0.2))\n",
    "\n",
    "my_model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "my_model.add(MaxPooling2D(pool_size=2))\n",
    "my_model.add(Dropout(0.2))\n",
    "\n",
    "my_model.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'))\n",
    "my_model.add(MaxPooling2D(pool_size=2))\n",
    "my_model.add(Dropout(0.2))\n",
    "\n",
    "my_model.add(Conv2D(filters=1024, kernel_size=3, padding='same', activation='relu'))\n",
    "my_model.add(MaxPooling2D(pool_size=2))\n",
    "my_model.add(Dropout(0.1))\n",
    "\n",
    "my_model.add(Conv2D(filters=2048, kernel_size=3, padding='same', activation='relu'))\n",
    "my_model.add(MaxPooling2D(pool_size=2))\n",
    "my_model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "my_model.add(GlobalAveragePooling2D())\n",
    "\n",
    "my_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "my_model.summary()\n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.25484451532746\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.25640431134822\n",
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/40\n",
      "2000/2000 [==============================] - ETA: 27:31 - loss: 1.0868 - acc: 0.36 - ETA: 25:09 - loss: 3.0417 - acc: 0.52 - ETA: 23:32 - loss: 3.4784 - acc: 0.59 - ETA: 21:53 - loss: 4.0997 - acc: 0.60 - ETA: 20:27 - loss: 4.0212 - acc: 0.63 - ETA: 18:59 - loss: 3.9957 - acc: 0.65 - ETA: 17:35 - loss: 4.0236 - acc: 0.66 - ETA: 16:15 - loss: 4.1855 - acc: 0.66 - ETA: 14:51 - loss: 4.3473 - acc: 0.66 - ETA: 13:31 - loss: 4.4928 - acc: 0.66 - ETA: 12:10 - loss: 4.6119 - acc: 0.66 - ETA: 10:50 - loss: 4.6439 - acc: 0.66 - ETA: 9:28 - loss: 4.7083 - acc: 0.6638 - ETA: 8:06 - loss: 4.7749 - acc: 0.662 - ETA: 6:45 - loss: 4.7574 - acc: 0.666 - ETA: 5:24 - loss: 4.7522 - acc: 0.669 - ETA: 4:02 - loss: 4.7951 - acc: 0.668 - ETA: 2:41 - loss: 4.7525 - acc: 0.673 - ETA: 1:20 - loss: 4.7569 - acc: 0.674 - 1650s 825ms/step - loss: 4.8253 - acc: 0.6720 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.73669, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 2/40\n",
      "2000/2000 [==============================] - ETA: 25:10 - loss: 4.8354 - acc: 0.70 - ETA: 23:54 - loss: 5.0772 - acc: 0.68 - ETA: 22:35 - loss: 5.1578 - acc: 0.68 - ETA: 21:17 - loss: 4.9563 - acc: 0.69 - ETA: 19:56 - loss: 4.7387 - acc: 0.70 - ETA: 18:36 - loss: 4.4862 - acc: 0.72 - ETA: 17:17 - loss: 4.4670 - acc: 0.72 - ETA: 16:00 - loss: 4.6742 - acc: 0.71 - ETA: 14:42 - loss: 4.7638 - acc: 0.70 - ETA: 13:23 - loss: 4.7871 - acc: 0.70 - ETA: 12:02 - loss: 4.8061 - acc: 0.70 - ETA: 10:43 - loss: 4.8757 - acc: 0.69 - ETA: 9:23 - loss: 4.8974 - acc: 0.6962 - ETA: 8:02 - loss: 4.9621 - acc: 0.692 - ETA: 6:41 - loss: 5.0074 - acc: 0.689 - ETA: 5:21 - loss: 4.9865 - acc: 0.690 - ETA: 4:00 - loss: 4.9776 - acc: 0.691 - ETA: 2:40 - loss: 4.9608 - acc: 0.692 - ETA: 1:20 - loss: 5.0136 - acc: 0.688 - 1636s 818ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 7.73669\n",
      "Epoch 3/40\n",
      "2000/2000 [==============================] - ETA: 25:03 - loss: 4.9966 - acc: 0.69 - ETA: 23:36 - loss: 5.1578 - acc: 0.68 - ETA: 22:15 - loss: 4.9966 - acc: 0.69 - ETA: 21:05 - loss: 5.1175 - acc: 0.68 - ETA: 19:50 - loss: 5.4479 - acc: 0.66 - ETA: 18:27 - loss: 5.1847 - acc: 0.67 - ETA: 17:10 - loss: 5.0887 - acc: 0.68 - ETA: 15:56 - loss: 5.1175 - acc: 0.68 - ETA: 14:36 - loss: 5.0145 - acc: 0.68 - ETA: 13:16 - loss: 4.8677 - acc: 0.69 - ETA: 11:57 - loss: 4.8940 - acc: 0.69 - ETA: 10:36 - loss: 4.9697 - acc: 0.69 - ETA: 9:17 - loss: 4.9470 - acc: 0.6931 - ETA: 7:57 - loss: 5.0427 - acc: 0.687 - ETA: 6:38 - loss: 5.0503 - acc: 0.686 - ETA: 5:18 - loss: 4.9765 - acc: 0.691 - ETA: 3:58 - loss: 5.0061 - acc: 0.689 - ETA: 2:39 - loss: 5.0235 - acc: 0.688 - ETA: 1:19 - loss: 5.0560 - acc: 0.686 - 1627s 813ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 7.73669\n",
      "Epoch 4/40\n",
      "2000/2000 [==============================] - ETA: 26:11 - loss: 4.8354 - acc: 0.70 - ETA: 24:22 - loss: 5.8831 - acc: 0.63 - ETA: 22:51 - loss: 5.7488 - acc: 0.64 - ETA: 21:25 - loss: 5.6816 - acc: 0.64 - ETA: 20:01 - loss: 5.4802 - acc: 0.66 - ETA: 18:41 - loss: 5.1309 - acc: 0.68 - ETA: 17:22 - loss: 4.9506 - acc: 0.69 - ETA: 16:02 - loss: 4.9966 - acc: 0.69 - ETA: 14:42 - loss: 4.9608 - acc: 0.69 - ETA: 13:21 - loss: 4.9966 - acc: 0.69 - ETA: 12:02 - loss: 4.9673 - acc: 0.69 - ETA: 10:42 - loss: 5.0100 - acc: 0.68 - ETA: 9:22 - loss: 4.9346 - acc: 0.6938 - ETA: 8:02 - loss: 4.9736 - acc: 0.691 - ETA: 6:42 - loss: 5.0181 - acc: 0.688 - ETA: 5:21 - loss: 5.0772 - acc: 0.685 - ETA: 4:01 - loss: 5.0345 - acc: 0.687 - ETA: 2:40 - loss: 5.0056 - acc: 0.689 - ETA: 1:20 - loss: 5.0221 - acc: 0.688 - 1643s 821ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 7.73669\n",
      "Epoch 5/40\n",
      "2000/2000 [==============================] - ETA: 25:33 - loss: 5.4802 - acc: 0.66 - ETA: 24:18 - loss: 5.6413 - acc: 0.65 - ETA: 22:49 - loss: 5.4802 - acc: 0.66 - ETA: 21:26 - loss: 5.5607 - acc: 0.65 - ETA: 20:02 - loss: 5.4157 - acc: 0.66 - ETA: 18:40 - loss: 5.2384 - acc: 0.67 - ETA: 17:25 - loss: 5.0657 - acc: 0.68 - ETA: 16:04 - loss: 5.0973 - acc: 0.68 - ETA: 14:50 - loss: 5.0324 - acc: 0.68 - ETA: 13:33 - loss: 5.0450 - acc: 0.68 - ETA: 12:10 - loss: 5.0259 - acc: 0.68 - ETA: 10:49 - loss: 4.9966 - acc: 0.69 - ETA: 9:28 - loss: 4.9718 - acc: 0.6915 - ETA: 8:07 - loss: 4.9966 - acc: 0.690 - ETA: 6:45 - loss: 5.0074 - acc: 0.689 - ETA: 5:24 - loss: 5.0268 - acc: 0.688 - ETA: 4:02 - loss: 4.9871 - acc: 0.690 - ETA: 2:41 - loss: 5.0414 - acc: 0.687 - ETA: 1:20 - loss: 5.0390 - acc: 0.687 - 1652s 826ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 7.73669\n",
      "Epoch 6/40\n",
      "2000/2000 [==============================] - ETA: 25:04 - loss: 4.0295 - acc: 0.75 - ETA: 23:56 - loss: 3.5460 - acc: 0.78 - ETA: 22:50 - loss: 4.0833 - acc: 0.74 - ETA: 21:36 - loss: 4.4728 - acc: 0.72 - ETA: 20:21 - loss: 4.8677 - acc: 0.69 - ETA: 19:00 - loss: 4.9697 - acc: 0.69 - ETA: 17:39 - loss: 4.8354 - acc: 0.70 - ETA: 16:20 - loss: 4.8556 - acc: 0.69 - ETA: 14:56 - loss: 4.8175 - acc: 0.70 - ETA: 13:33 - loss: 4.8677 - acc: 0.69 - ETA: 12:10 - loss: 4.9233 - acc: 0.69 - ETA: 10:48 - loss: 4.8757 - acc: 0.69 - ETA: 9:27 - loss: 4.8974 - acc: 0.6962 - ETA: 8:05 - loss: 5.0196 - acc: 0.688 - ETA: 6:44 - loss: 5.0611 - acc: 0.686 - ETA: 5:23 - loss: 5.0873 - acc: 0.684 - ETA: 4:02 - loss: 5.0819 - acc: 0.684 - ETA: 2:41 - loss: 5.0682 - acc: 0.685 - ETA: 1:20 - loss: 5.1323 - acc: 0.681 - 1650s 825ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 7.73669\n",
      "Epoch 7/40\n",
      "2000/2000 [==============================] - ETA: 25:28 - loss: 4.8354 - acc: 0.70 - ETA: 23:57 - loss: 4.7548 - acc: 0.70 - ETA: 22:38 - loss: 4.7280 - acc: 0.70 - ETA: 21:26 - loss: 4.4325 - acc: 0.72 - ETA: 20:05 - loss: 4.6742 - acc: 0.71 - ETA: 18:44 - loss: 4.8623 - acc: 0.69 - ETA: 17:26 - loss: 4.9506 - acc: 0.69 - ETA: 16:06 - loss: 4.9966 - acc: 0.69 - ETA: 14:45 - loss: 4.8892 - acc: 0.69 - ETA: 13:23 - loss: 4.8677 - acc: 0.69 - ETA: 12:01 - loss: 4.8501 - acc: 0.69 - ETA: 10:41 - loss: 4.8757 - acc: 0.69 - ETA: 9:22 - loss: 4.8726 - acc: 0.6977 - ETA: 8:01 - loss: 4.9390 - acc: 0.693 - ETA: 6:41 - loss: 4.9644 - acc: 0.692 - ETA: 5:20 - loss: 4.9563 - acc: 0.692 - ETA: 4:00 - loss: 4.9776 - acc: 0.691 - ETA: 2:40 - loss: 4.9787 - acc: 0.691 - ETA: 1:20 - loss: 5.0390 - acc: 0.687 - 1642s 821ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 7.73669\n",
      "Epoch 8/40\n",
      "2000/2000 [==============================] - ETA: 25:17 - loss: 5.6413 - acc: 0.65 - ETA: 24:23 - loss: 5.1578 - acc: 0.68 - ETA: 23:22 - loss: 5.4802 - acc: 0.66 - ETA: 21:52 - loss: 5.5607 - acc: 0.65 - ETA: 20:28 - loss: 5.5769 - acc: 0.65 - ETA: 19:00 - loss: 5.6951 - acc: 0.64 - ETA: 17:39 - loss: 5.6183 - acc: 0.65 - ETA: 16:15 - loss: 5.4197 - acc: 0.66 - ETA: 14:54 - loss: 5.4622 - acc: 0.66 - ETA: 13:32 - loss: 5.3996 - acc: 0.66 - ETA: 12:10 - loss: 5.4215 - acc: 0.66 - ETA: 10:49 - loss: 5.3055 - acc: 0.67 - ETA: 9:29 - loss: 5.1330 - acc: 0.6815 - ETA: 8:08 - loss: 5.1002 - acc: 0.683 - ETA: 6:46 - loss: 5.0718 - acc: 0.685 - ETA: 5:25 - loss: 5.0369 - acc: 0.687 - ETA: 4:03 - loss: 5.0630 - acc: 0.685 - ETA: 2:42 - loss: 5.0593 - acc: 0.686 - ETA: 1:21 - loss: 5.0645 - acc: 0.685 - 1661s 831ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 7.73669\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 25:18 - loss: 4.9966 - acc: 0.69 - ETA: 24:36 - loss: 5.5607 - acc: 0.65 - ETA: 23:05 - loss: 4.9966 - acc: 0.69 - ETA: 21:41 - loss: 5.0369 - acc: 0.68 - ETA: 20:23 - loss: 5.0933 - acc: 0.68 - ETA: 19:00 - loss: 4.9966 - acc: 0.69 - ETA: 17:38 - loss: 4.9506 - acc: 0.69 - ETA: 16:15 - loss: 5.0168 - acc: 0.68 - ETA: 14:53 - loss: 5.0324 - acc: 0.68 - ETA: 13:32 - loss: 5.0450 - acc: 0.68 - ETA: 12:12 - loss: 4.9966 - acc: 0.69 - ETA: 10:49 - loss: 4.9295 - acc: 0.69 - ETA: 9:27 - loss: 4.9098 - acc: 0.6954 - ETA: 8:06 - loss: 4.9390 - acc: 0.693 - ETA: 6:45 - loss: 4.9429 - acc: 0.693 - ETA: 5:23 - loss: 4.9865 - acc: 0.690 - ETA: 4:02 - loss: 4.9682 - acc: 0.691 - ETA: 2:41 - loss: 4.9787 - acc: 0.691 - ETA: 1:21 - loss: 5.0645 - acc: 0.685 - 1660s 830ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 7.73669\n",
      "Epoch 10/40\n",
      "2000/2000 [==============================] - ETA: 25:23 - loss: 5.1578 - acc: 0.68 - ETA: 24:01 - loss: 5.1578 - acc: 0.68 - ETA: 22:42 - loss: 5.3190 - acc: 0.67 - ETA: 21:30 - loss: 5.1578 - acc: 0.68 - ETA: 20:16 - loss: 4.9644 - acc: 0.69 - ETA: 18:58 - loss: 5.0772 - acc: 0.68 - ETA: 17:36 - loss: 5.0657 - acc: 0.68 - ETA: 16:15 - loss: 4.9765 - acc: 0.69 - ETA: 14:54 - loss: 4.8354 - acc: 0.70 - ETA: 13:34 - loss: 4.8193 - acc: 0.70 - ETA: 12:14 - loss: 4.8794 - acc: 0.69 - ETA: 10:54 - loss: 4.9429 - acc: 0.69 - ETA: 9:32 - loss: 4.9346 - acc: 0.6938 - ETA: 8:10 - loss: 4.9851 - acc: 0.690 - ETA: 6:48 - loss: 5.0181 - acc: 0.688 - ETA: 5:26 - loss: 4.9865 - acc: 0.690 - ETA: 4:04 - loss: 5.0440 - acc: 0.687 - ETA: 2:43 - loss: 5.0503 - acc: 0.686 - ETA: 1:21 - loss: 5.0899 - acc: 0.684 - 1665s 832ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 7.73669\n",
      "Epoch 11/40\n",
      "2000/2000 [==============================] - ETA: 25:46 - loss: 5.6413 - acc: 0.65 - ETA: 24:14 - loss: 5.8831 - acc: 0.63 - ETA: 23:10 - loss: 5.4264 - acc: 0.66 - ETA: 21:43 - loss: 5.1981 - acc: 0.67 - ETA: 20:28 - loss: 5.0611 - acc: 0.68 - ETA: 19:04 - loss: 5.0235 - acc: 0.68 - ETA: 17:40 - loss: 5.1808 - acc: 0.67 - ETA: 16:17 - loss: 5.1578 - acc: 0.68 - ETA: 14:59 - loss: 5.0862 - acc: 0.68 - ETA: 13:34 - loss: 5.0933 - acc: 0.68 - ETA: 12:14 - loss: 5.1578 - acc: 0.68 - ETA: 10:51 - loss: 5.0906 - acc: 0.68 - ETA: 9:28 - loss: 5.0586 - acc: 0.6862 - ETA: 8:08 - loss: 5.0081 - acc: 0.689 - ETA: 6:47 - loss: 5.0503 - acc: 0.686 - ETA: 5:25 - loss: 5.0470 - acc: 0.686 - ETA: 4:04 - loss: 5.0630 - acc: 0.685 - ETA: 2:42 - loss: 4.9966 - acc: 0.690 - ETA: 1:21 - loss: 4.9712 - acc: 0.691 - 1662s 831ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 7.73669\n",
      "Epoch 12/40\n",
      "2000/2000 [==============================] - ETA: 25:41 - loss: 4.6742 - acc: 0.71 - ETA: 24:04 - loss: 4.8354 - acc: 0.70 - ETA: 22:45 - loss: 4.7817 - acc: 0.70 - ETA: 21:21 - loss: 4.7548 - acc: 0.70 - ETA: 20:03 - loss: 4.8354 - acc: 0.70 - ETA: 18:43 - loss: 4.8623 - acc: 0.69 - ETA: 17:26 - loss: 4.9275 - acc: 0.69 - ETA: 16:07 - loss: 4.8959 - acc: 0.69 - ETA: 14:49 - loss: 4.9071 - acc: 0.69 - ETA: 13:26 - loss: 4.9483 - acc: 0.69 - ETA: 12:07 - loss: 4.9820 - acc: 0.69 - ETA: 10:45 - loss: 4.9966 - acc: 0.69 - ETA: 9:26 - loss: 4.9842 - acc: 0.6908 - ETA: 8:04 - loss: 4.9966 - acc: 0.690 - ETA: 6:44 - loss: 5.0826 - acc: 0.684 - ETA: 5:23 - loss: 5.0772 - acc: 0.685 - ETA: 4:02 - loss: 5.0914 - acc: 0.684 - ETA: 2:41 - loss: 5.1220 - acc: 0.682 - ETA: 1:20 - loss: 5.0730 - acc: 0.685 - 1649s 825ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 7.73669\n",
      "Epoch 13/40\n",
      "2000/2000 [==============================] - ETA: 25:15 - loss: 5.9637 - acc: 0.63 - ETA: 23:51 - loss: 5.8025 - acc: 0.64 - ETA: 22:50 - loss: 5.6413 - acc: 0.65 - ETA: 21:38 - loss: 5.1578 - acc: 0.68 - ETA: 20:13 - loss: 5.3834 - acc: 0.66 - ETA: 18:55 - loss: 5.2384 - acc: 0.67 - ETA: 17:31 - loss: 5.1578 - acc: 0.68 - ETA: 16:14 - loss: 5.2182 - acc: 0.67 - ETA: 14:51 - loss: 5.1399 - acc: 0.68 - ETA: 13:29 - loss: 5.1256 - acc: 0.68 - ETA: 12:07 - loss: 5.0406 - acc: 0.68 - ETA: 10:48 - loss: 5.0369 - acc: 0.68 - ETA: 9:26 - loss: 4.9966 - acc: 0.6900 - ETA: 8:06 - loss: 4.9966 - acc: 0.690 - ETA: 6:45 - loss: 4.9429 - acc: 0.693 - ETA: 5:23 - loss: 4.9563 - acc: 0.692 - ETA: 4:02 - loss: 5.0251 - acc: 0.688 - ETA: 2:41 - loss: 5.0145 - acc: 0.688 - ETA: 1:20 - loss: 4.9796 - acc: 0.691 - 1650s 825ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 7.73669\n",
      "Epoch 14/40\n",
      "2000/2000 [==============================] - ETA: 24:49 - loss: 4.6742 - acc: 0.71 - ETA: 23:25 - loss: 4.7548 - acc: 0.70 - ETA: 22:07 - loss: 4.9429 - acc: 0.69 - ETA: 20:44 - loss: 4.7548 - acc: 0.70 - ETA: 19:22 - loss: 4.7387 - acc: 0.70 - ETA: 18:04 - loss: 4.9697 - acc: 0.69 - ETA: 16:49 - loss: 4.9736 - acc: 0.69 - ETA: 15:33 - loss: 4.9362 - acc: 0.69 - ETA: 14:18 - loss: 4.9429 - acc: 0.69 - ETA: 13:01 - loss: 4.8193 - acc: 0.70 - ETA: 11:43 - loss: 4.8061 - acc: 0.70 - ETA: 10:28 - loss: 4.8489 - acc: 0.69 - ETA: 9:10 - loss: 4.8230 - acc: 0.7008 - ETA: 7:51 - loss: 4.8009 - acc: 0.702 - ETA: 6:33 - loss: 4.8354 - acc: 0.700 - ETA: 5:15 - loss: 4.8959 - acc: 0.696 - ETA: 3:56 - loss: 4.9492 - acc: 0.692 - ETA: 2:38 - loss: 4.9071 - acc: 0.695 - ETA: 1:19 - loss: 4.9542 - acc: 0.692 - 1618s 809ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 7.73669\n",
      "Epoch 15/40\n",
      "2000/2000 [==============================] - ETA: 25:05 - loss: 5.6413 - acc: 0.65 - ETA: 23:53 - loss: 5.3190 - acc: 0.67 - ETA: 22:40 - loss: 5.3190 - acc: 0.67 - ETA: 21:29 - loss: 5.1175 - acc: 0.68 - ETA: 20:06 - loss: 5.2223 - acc: 0.67 - ETA: 18:56 - loss: 5.2921 - acc: 0.67 - ETA: 17:33 - loss: 5.0887 - acc: 0.68 - ETA: 16:17 - loss: 5.2988 - acc: 0.67 - ETA: 14:55 - loss: 5.1757 - acc: 0.67 - ETA: 13:35 - loss: 5.2223 - acc: 0.67 - ETA: 12:13 - loss: 5.1871 - acc: 0.67 - ETA: 10:53 - loss: 5.1175 - acc: 0.68 - ETA: 9:32 - loss: 5.1206 - acc: 0.6823 - ETA: 8:10 - loss: 5.1463 - acc: 0.680 - ETA: 6:48 - loss: 5.1578 - acc: 0.680 - ETA: 5:26 - loss: 5.1679 - acc: 0.679 - ETA: 4:04 - loss: 5.1293 - acc: 0.681 - ETA: 2:43 - loss: 5.1488 - acc: 0.680 - ETA: 1:21 - loss: 5.1239 - acc: 0.682 - 1665s 833ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 7.73669\n",
      "Epoch 16/40\n",
      "2000/2000 [==============================] - ETA: 25:44 - loss: 3.5460 - acc: 0.78 - ETA: 24:18 - loss: 4.1907 - acc: 0.74 - ETA: 22:55 - loss: 4.8354 - acc: 0.70 - ETA: 21:33 - loss: 4.9563 - acc: 0.69 - ETA: 20:13 - loss: 5.0933 - acc: 0.68 - ETA: 19:02 - loss: 5.1309 - acc: 0.68 - ETA: 17:46 - loss: 4.9966 - acc: 0.69 - ETA: 16:25 - loss: 4.9966 - acc: 0.69 - ETA: 15:09 - loss: 5.0145 - acc: 0.68 - ETA: 13:44 - loss: 4.9160 - acc: 0.69 - ETA: 12:19 - loss: 4.8794 - acc: 0.69 - ETA: 10:56 - loss: 4.9295 - acc: 0.69 - ETA: 9:33 - loss: 4.9098 - acc: 0.6954 - ETA: 8:13 - loss: 4.8815 - acc: 0.697 - ETA: 6:51 - loss: 4.9751 - acc: 0.691 - ETA: 5:29 - loss: 5.0369 - acc: 0.687 - ETA: 4:07 - loss: 5.0535 - acc: 0.686 - ETA: 2:44 - loss: 5.0324 - acc: 0.687 - ETA: 1:22 - loss: 5.0645 - acc: 0.685 - 1684s 842ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 7.73669\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 26:12 - loss: 4.9966 - acc: 0.69 - ETA: 24:38 - loss: 4.9966 - acc: 0.69 - ETA: 23:14 - loss: 4.6742 - acc: 0.71 - ETA: 21:49 - loss: 4.9160 - acc: 0.69 - ETA: 20:22 - loss: 5.2545 - acc: 0.67 - ETA: 19:01 - loss: 5.2652 - acc: 0.67 - ETA: 17:41 - loss: 5.0196 - acc: 0.68 - ETA: 16:17 - loss: 5.0571 - acc: 0.68 - ETA: 14:56 - loss: 5.1041 - acc: 0.68 - ETA: 13:33 - loss: 5.1900 - acc: 0.67 - ETA: 12:13 - loss: 5.1871 - acc: 0.67 - ETA: 10:50 - loss: 5.1309 - acc: 0.68 - ETA: 9:29 - loss: 5.1454 - acc: 0.6808 - ETA: 8:08 - loss: 5.1348 - acc: 0.681 - ETA: 6:47 - loss: 5.1793 - acc: 0.678 - ETA: 5:26 - loss: 5.1477 - acc: 0.680 - ETA: 4:04 - loss: 5.1483 - acc: 0.680 - ETA: 2:43 - loss: 5.0593 - acc: 0.686 - ETA: 1:21 - loss: 5.0560 - acc: 0.686 - 1669s 835ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 7.73669\n",
      "Epoch 18/40\n",
      "2000/2000 [==============================] - ETA: 26:04 - loss: 5.1578 - acc: 0.68 - ETA: 24:36 - loss: 5.2384 - acc: 0.67 - ETA: 23:13 - loss: 4.8354 - acc: 0.70 - ETA: 21:51 - loss: 4.8354 - acc: 0.70 - ETA: 20:27 - loss: 5.1256 - acc: 0.68 - ETA: 19:05 - loss: 5.1041 - acc: 0.68 - ETA: 17:43 - loss: 5.1117 - acc: 0.68 - ETA: 16:21 - loss: 5.0168 - acc: 0.68 - ETA: 14:58 - loss: 4.9429 - acc: 0.69 - ETA: 13:42 - loss: 5.0450 - acc: 0.68 - ETA: 12:19 - loss: 5.0992 - acc: 0.68 - ETA: 10:58 - loss: 5.0906 - acc: 0.68 - ETA: 9:36 - loss: 5.0958 - acc: 0.6838 - ETA: 8:13 - loss: 5.0772 - acc: 0.685 - ETA: 6:51 - loss: 5.0718 - acc: 0.685 - ETA: 5:29 - loss: 5.1175 - acc: 0.682 - ETA: 4:07 - loss: 5.0345 - acc: 0.687 - ETA: 2:45 - loss: 5.0682 - acc: 0.685 - ETA: 1:22 - loss: 5.0899 - acc: 0.684 - 1691s 845ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 7.73669\n",
      "Epoch 19/40\n",
      "2000/2000 [==============================] - ETA: 25:24 - loss: 4.9966 - acc: 0.69 - ETA: 24:18 - loss: 4.9160 - acc: 0.69 - ETA: 23:13 - loss: 4.9429 - acc: 0.69 - ETA: 21:49 - loss: 5.0369 - acc: 0.68 - ETA: 20:19 - loss: 5.1256 - acc: 0.68 - ETA: 18:54 - loss: 5.0503 - acc: 0.68 - ETA: 17:39 - loss: 4.9966 - acc: 0.69 - ETA: 16:19 - loss: 4.9362 - acc: 0.69 - ETA: 14:55 - loss: 4.9250 - acc: 0.69 - ETA: 13:33 - loss: 4.9644 - acc: 0.69 - ETA: 12:13 - loss: 4.9380 - acc: 0.69 - ETA: 10:53 - loss: 5.0235 - acc: 0.68 - ETA: 9:31 - loss: 4.9842 - acc: 0.6908 - ETA: 8:09 - loss: 4.8930 - acc: 0.696 - ETA: 6:47 - loss: 4.9966 - acc: 0.690 - ETA: 5:26 - loss: 5.0470 - acc: 0.686 - ETA: 4:04 - loss: 5.0914 - acc: 0.684 - ETA: 2:43 - loss: 5.1399 - acc: 0.681 - ETA: 1:21 - loss: 5.0899 - acc: 0.684 - 1667s 833ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 7.73669\n",
      "Epoch 20/40\n",
      "2000/2000 [==============================] - ETA: 26:10 - loss: 5.6413 - acc: 0.65 - ETA: 24:39 - loss: 5.6413 - acc: 0.65 - ETA: 23:04 - loss: 5.5339 - acc: 0.65 - ETA: 21:35 - loss: 5.4399 - acc: 0.66 - ETA: 20:23 - loss: 5.2223 - acc: 0.67 - ETA: 19:02 - loss: 4.9697 - acc: 0.69 - ETA: 17:39 - loss: 4.9736 - acc: 0.69 - ETA: 16:21 - loss: 4.8959 - acc: 0.69 - ETA: 14:58 - loss: 5.0682 - acc: 0.68 - ETA: 13:37 - loss: 4.9966 - acc: 0.69 - ETA: 12:16 - loss: 4.9820 - acc: 0.69 - ETA: 10:55 - loss: 4.9295 - acc: 0.69 - ETA: 9:33 - loss: 4.8602 - acc: 0.6985 - ETA: 8:12 - loss: 4.9045 - acc: 0.695 - ETA: 6:50 - loss: 4.8999 - acc: 0.696 - ETA: 5:27 - loss: 4.9462 - acc: 0.693 - ETA: 4:05 - loss: 4.9682 - acc: 0.691 - ETA: 2:43 - loss: 4.9608 - acc: 0.692 - ETA: 1:21 - loss: 4.9881 - acc: 0.690 - 1668s 834ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 7.73669\n",
      "Epoch 21/40\n",
      "2000/2000 [==============================] - ETA: 26:03 - loss: 5.3190 - acc: 0.67 - ETA: 24:24 - loss: 5.3996 - acc: 0.66 - ETA: 22:56 - loss: 5.2115 - acc: 0.67 - ETA: 21:33 - loss: 5.1578 - acc: 0.68 - ETA: 20:12 - loss: 5.0933 - acc: 0.68 - ETA: 18:51 - loss: 4.8623 - acc: 0.69 - ETA: 17:30 - loss: 4.9045 - acc: 0.69 - ETA: 16:08 - loss: 4.9362 - acc: 0.69 - ETA: 14:48 - loss: 5.0682 - acc: 0.68 - ETA: 13:27 - loss: 5.1417 - acc: 0.68 - ETA: 12:06 - loss: 5.0992 - acc: 0.68 - ETA: 10:45 - loss: 5.0638 - acc: 0.68 - ETA: 9:24 - loss: 5.0586 - acc: 0.6862 - ETA: 8:04 - loss: 5.0311 - acc: 0.687 - ETA: 6:43 - loss: 5.0611 - acc: 0.686 - ETA: 5:23 - loss: 5.0067 - acc: 0.689 - ETA: 4:03 - loss: 5.0156 - acc: 0.688 - ETA: 2:42 - loss: 5.0414 - acc: 0.687 - ETA: 1:21 - loss: 5.0899 - acc: 0.684 - 1657s 828ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 7.73669\n",
      "Epoch 22/40\n",
      "2000/2000 [==============================] - ETA: 26:27 - loss: 5.4802 - acc: 0.66 - ETA: 24:39 - loss: 5.9637 - acc: 0.63 - ETA: 23:10 - loss: 5.8562 - acc: 0.63 - ETA: 21:58 - loss: 5.4802 - acc: 0.66 - ETA: 20:31 - loss: 5.2545 - acc: 0.67 - ETA: 19:07 - loss: 5.2384 - acc: 0.67 - ETA: 17:44 - loss: 5.1808 - acc: 0.67 - ETA: 16:26 - loss: 5.2384 - acc: 0.67 - ETA: 15:05 - loss: 5.1757 - acc: 0.67 - ETA: 13:44 - loss: 5.1417 - acc: 0.68 - ETA: 12:20 - loss: 5.0699 - acc: 0.68 - ETA: 10:58 - loss: 4.9832 - acc: 0.69 - ETA: 9:34 - loss: 4.9966 - acc: 0.6900 - ETA: 8:12 - loss: 4.9506 - acc: 0.692 - ETA: 6:49 - loss: 4.9536 - acc: 0.692 - ETA: 5:28 - loss: 4.9664 - acc: 0.691 - ETA: 4:05 - loss: 5.0345 - acc: 0.687 - ETA: 2:43 - loss: 5.0145 - acc: 0.688 - ETA: 1:21 - loss: 5.0305 - acc: 0.687 - 1668s 834ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 7.73669\n",
      "Epoch 23/40\n",
      "2000/2000 [==============================] - ETA: 25:29 - loss: 5.4802 - acc: 0.66 - ETA: 24:04 - loss: 5.5607 - acc: 0.65 - ETA: 22:45 - loss: 5.3190 - acc: 0.67 - ETA: 21:34 - loss: 5.3593 - acc: 0.66 - ETA: 20:23 - loss: 5.2223 - acc: 0.67 - ETA: 18:59 - loss: 5.3190 - acc: 0.67 - ETA: 17:39 - loss: 5.2729 - acc: 0.67 - ETA: 16:22 - loss: 5.1376 - acc: 0.68 - ETA: 15:02 - loss: 5.1220 - acc: 0.68 - ETA: 13:39 - loss: 5.1094 - acc: 0.68 - ETA: 12:14 - loss: 4.9820 - acc: 0.69 - ETA: 10:52 - loss: 5.0369 - acc: 0.68 - ETA: 9:29 - loss: 5.0338 - acc: 0.6877 - ETA: 8:08 - loss: 5.0772 - acc: 0.685 - ETA: 6:47 - loss: 5.1148 - acc: 0.682 - ETA: 5:25 - loss: 5.0873 - acc: 0.684 - ETA: 4:03 - loss: 5.0725 - acc: 0.685 - ETA: 2:42 - loss: 5.0682 - acc: 0.685 - ETA: 1:21 - loss: 5.1154 - acc: 0.682 - 1657s 829ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 7.73669\n",
      "Epoch 24/40\n",
      "2000/2000 [==============================] - ETA: 25:45 - loss: 4.9966 - acc: 0.69 - ETA: 24:04 - loss: 4.5937 - acc: 0.71 - ETA: 22:36 - loss: 4.6205 - acc: 0.71 - ETA: 21:19 - loss: 4.7548 - acc: 0.70 - ETA: 19:59 - loss: 5.0933 - acc: 0.68 - ETA: 18:48 - loss: 4.9966 - acc: 0.69 - ETA: 17:26 - loss: 5.2729 - acc: 0.67 - ETA: 16:06 - loss: 5.1779 - acc: 0.67 - ETA: 14:50 - loss: 5.0145 - acc: 0.68 - ETA: 13:27 - loss: 5.1417 - acc: 0.68 - ETA: 12:05 - loss: 5.1138 - acc: 0.68 - ETA: 10:44 - loss: 5.0906 - acc: 0.68 - ETA: 9:26 - loss: 5.1082 - acc: 0.6831 - ETA: 8:04 - loss: 5.1117 - acc: 0.682 - ETA: 6:45 - loss: 5.0503 - acc: 0.686 - ETA: 5:24 - loss: 5.0470 - acc: 0.686 - ETA: 4:03 - loss: 5.0440 - acc: 0.687 - ETA: 2:42 - loss: 5.0951 - acc: 0.683 - ETA: 1:21 - loss: 5.1578 - acc: 0.680 - 1666s 833ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 7.73669\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 25:43 - loss: 5.4802 - acc: 0.66 - ETA: 24:05 - loss: 5.2384 - acc: 0.67 - ETA: 22:54 - loss: 5.3727 - acc: 0.66 - ETA: 21:31 - loss: 5.5204 - acc: 0.65 - ETA: 20:22 - loss: 5.3512 - acc: 0.66 - ETA: 18:56 - loss: 5.1578 - acc: 0.68 - ETA: 17:35 - loss: 5.1348 - acc: 0.68 - ETA: 16:17 - loss: 5.0973 - acc: 0.68 - ETA: 14:54 - loss: 5.0324 - acc: 0.68 - ETA: 13:32 - loss: 4.9805 - acc: 0.69 - ETA: 12:10 - loss: 5.0845 - acc: 0.68 - ETA: 10:50 - loss: 5.1175 - acc: 0.68 - ETA: 9:28 - loss: 5.1330 - acc: 0.6815 - ETA: 8:06 - loss: 5.0887 - acc: 0.684 - ETA: 6:46 - loss: 5.0826 - acc: 0.684 - ETA: 5:25 - loss: 5.0772 - acc: 0.685 - ETA: 4:04 - loss: 5.0819 - acc: 0.684 - ETA: 2:42 - loss: 5.0951 - acc: 0.683 - ETA: 1:21 - loss: 5.0814 - acc: 0.684 - 1662s 831ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 7.73669\n",
      "Epoch 26/40\n",
      "2000/2000 [==============================] - ETA: 25:31 - loss: 5.8025 - acc: 0.64 - ETA: 23:57 - loss: 5.8831 - acc: 0.63 - ETA: 23:08 - loss: 5.8025 - acc: 0.64 - ETA: 21:37 - loss: 5.6010 - acc: 0.65 - ETA: 20:23 - loss: 5.4802 - acc: 0.66 - ETA: 19:03 - loss: 5.3458 - acc: 0.66 - ETA: 17:44 - loss: 5.3650 - acc: 0.66 - ETA: 16:17 - loss: 5.3391 - acc: 0.66 - ETA: 14:53 - loss: 5.2832 - acc: 0.67 - ETA: 13:30 - loss: 5.2384 - acc: 0.67 - ETA: 12:07 - loss: 5.2164 - acc: 0.67 - ETA: 10:45 - loss: 5.1578 - acc: 0.68 - ETA: 9:24 - loss: 5.2198 - acc: 0.6762 - ETA: 8:04 - loss: 5.1578 - acc: 0.680 - ETA: 6:44 - loss: 5.1470 - acc: 0.680 - ETA: 5:23 - loss: 5.0369 - acc: 0.687 - ETA: 4:03 - loss: 5.0535 - acc: 0.686 - ETA: 2:42 - loss: 5.0951 - acc: 0.683 - ETA: 1:21 - loss: 5.0730 - acc: 0.685 - 1662s 831ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 7.73669\n",
      "Epoch 27/40\n",
      "2000/2000 [==============================] - ETA: 26:42 - loss: 5.1578 - acc: 0.68 - ETA: 24:31 - loss: 5.7219 - acc: 0.64 - ETA: 22:57 - loss: 5.5339 - acc: 0.65 - ETA: 21:47 - loss: 5.4802 - acc: 0.66 - ETA: 20:31 - loss: 5.3834 - acc: 0.66 - ETA: 19:02 - loss: 5.1847 - acc: 0.67 - ETA: 17:38 - loss: 5.2269 - acc: 0.67 - ETA: 16:14 - loss: 5.2384 - acc: 0.67 - ETA: 14:52 - loss: 5.1578 - acc: 0.68 - ETA: 13:29 - loss: 5.1256 - acc: 0.68 - ETA: 12:11 - loss: 5.1724 - acc: 0.67 - ETA: 10:50 - loss: 5.0638 - acc: 0.68 - ETA: 9:30 - loss: 5.0586 - acc: 0.6862 - ETA: 8:08 - loss: 5.0772 - acc: 0.685 - ETA: 6:46 - loss: 5.0718 - acc: 0.685 - ETA: 5:26 - loss: 4.9966 - acc: 0.690 - ETA: 4:04 - loss: 4.9776 - acc: 0.691 - ETA: 2:42 - loss: 5.0056 - acc: 0.689 - ETA: 1:21 - loss: 5.0814 - acc: 0.684 - 1661s 831ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 7.73669\n",
      "Epoch 28/40\n",
      "2000/2000 [==============================] - ETA: 25:52 - loss: 3.8683 - acc: 0.76 - ETA: 24:06 - loss: 4.9966 - acc: 0.69 - ETA: 23:07 - loss: 5.3727 - acc: 0.66 - ETA: 21:53 - loss: 5.3190 - acc: 0.67 - ETA: 20:24 - loss: 5.2545 - acc: 0.67 - ETA: 19:06 - loss: 5.1847 - acc: 0.67 - ETA: 17:46 - loss: 5.0657 - acc: 0.68 - ETA: 16:23 - loss: 5.1578 - acc: 0.68 - ETA: 14:58 - loss: 5.2294 - acc: 0.67 - ETA: 13:35 - loss: 5.2223 - acc: 0.67 - ETA: 12:15 - loss: 5.1871 - acc: 0.67 - ETA: 10:53 - loss: 5.1444 - acc: 0.68 - ETA: 9:30 - loss: 5.1454 - acc: 0.6808 - ETA: 8:09 - loss: 5.1693 - acc: 0.679 - ETA: 6:47 - loss: 5.1256 - acc: 0.682 - ETA: 5:26 - loss: 5.0873 - acc: 0.684 - ETA: 4:04 - loss: 5.0725 - acc: 0.685 - ETA: 2:42 - loss: 5.0862 - acc: 0.684 - ETA: 1:21 - loss: 5.0475 - acc: 0.686 - 1660s 830ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 7.73669\n",
      "Epoch 29/40\n",
      "2000/2000 [==============================] - ETA: 25:38 - loss: 4.9966 - acc: 0.69 - ETA: 24:12 - loss: 5.4802 - acc: 0.66 - ETA: 23:06 - loss: 5.4264 - acc: 0.66 - ETA: 21:35 - loss: 5.2787 - acc: 0.67 - ETA: 20:20 - loss: 5.0933 - acc: 0.68 - ETA: 18:53 - loss: 4.9966 - acc: 0.69 - ETA: 17:30 - loss: 4.9966 - acc: 0.69 - ETA: 16:11 - loss: 5.0369 - acc: 0.68 - ETA: 14:49 - loss: 4.9787 - acc: 0.69 - ETA: 13:31 - loss: 4.9483 - acc: 0.69 - ETA: 12:10 - loss: 5.0992 - acc: 0.68 - ETA: 10:47 - loss: 5.0369 - acc: 0.68 - ETA: 9:27 - loss: 5.0462 - acc: 0.6869 - ETA: 8:07 - loss: 5.0542 - acc: 0.686 - ETA: 6:45 - loss: 5.1041 - acc: 0.683 - ETA: 5:24 - loss: 5.0671 - acc: 0.685 - ETA: 4:02 - loss: 5.0819 - acc: 0.684 - ETA: 2:41 - loss: 5.0324 - acc: 0.687 - ETA: 1:21 - loss: 5.0814 - acc: 0.684 - 1654s 827ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 7.73669\n",
      "Epoch 30/40\n",
      "2000/2000 [==============================] - ETA: 27:01 - loss: 5.3190 - acc: 0.67 - ETA: 25:03 - loss: 5.4802 - acc: 0.66 - ETA: 23:37 - loss: 5.3190 - acc: 0.67 - ETA: 22:00 - loss: 5.2787 - acc: 0.67 - ETA: 20:31 - loss: 5.0933 - acc: 0.68 - ETA: 19:12 - loss: 4.9966 - acc: 0.69 - ETA: 17:48 - loss: 5.0427 - acc: 0.68 - ETA: 16:25 - loss: 5.0168 - acc: 0.68 - ETA: 14:59 - loss: 4.9966 - acc: 0.69 - ETA: 13:34 - loss: 5.0450 - acc: 0.68 - ETA: 12:14 - loss: 5.0406 - acc: 0.68 - ETA: 10:52 - loss: 5.1578 - acc: 0.68 - ETA: 9:29 - loss: 5.1082 - acc: 0.6831 - ETA: 8:07 - loss: 5.1002 - acc: 0.683 - ETA: 6:45 - loss: 5.0933 - acc: 0.684 - ETA: 5:25 - loss: 5.1376 - acc: 0.681 - ETA: 4:03 - loss: 5.0914 - acc: 0.684 - ETA: 2:42 - loss: 5.1130 - acc: 0.682 - ETA: 1:21 - loss: 5.0475 - acc: 0.686 - 1658s 829ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 7.73669\n",
      "Epoch 31/40\n",
      "2000/2000 [==============================] - ETA: 25:13 - loss: 4.6742 - acc: 0.71 - ETA: 24:12 - loss: 4.5937 - acc: 0.71 - ETA: 22:46 - loss: 4.9966 - acc: 0.69 - ETA: 21:41 - loss: 4.8757 - acc: 0.69 - ETA: 20:14 - loss: 4.9321 - acc: 0.69 - ETA: 18:49 - loss: 4.8892 - acc: 0.69 - ETA: 17:33 - loss: 4.7664 - acc: 0.70 - ETA: 16:10 - loss: 4.7951 - acc: 0.70 - ETA: 14:46 - loss: 4.8175 - acc: 0.70 - ETA: 13:29 - loss: 4.8999 - acc: 0.69 - ETA: 12:07 - loss: 4.9380 - acc: 0.69 - ETA: 10:47 - loss: 4.9563 - acc: 0.69 - ETA: 9:25 - loss: 5.0586 - acc: 0.6862 - ETA: 8:05 - loss: 5.0887 - acc: 0.684 - ETA: 6:44 - loss: 5.0933 - acc: 0.684 - ETA: 5:23 - loss: 5.0168 - acc: 0.688 - ETA: 4:02 - loss: 5.0630 - acc: 0.685 - ETA: 2:41 - loss: 5.0593 - acc: 0.686 - ETA: 1:20 - loss: 5.0390 - acc: 0.687 - 1645s 823ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 7.73669\n",
      "Epoch 32/40\n",
      "2000/2000 [==============================] - ETA: 25:00 - loss: 4.8354 - acc: 0.70 - ETA: 23:33 - loss: 5.3996 - acc: 0.66 - ETA: 22:39 - loss: 5.8025 - acc: 0.64 - ETA: 21:37 - loss: 5.6413 - acc: 0.65 - ETA: 20:09 - loss: 5.3834 - acc: 0.66 - ETA: 18:43 - loss: 5.2921 - acc: 0.67 - ETA: 17:20 - loss: 5.2729 - acc: 0.67 - ETA: 15:59 - loss: 5.2384 - acc: 0.67 - ETA: 14:37 - loss: 5.1936 - acc: 0.67 - ETA: 13:16 - loss: 5.2384 - acc: 0.67 - ETA: 11:59 - loss: 5.1871 - acc: 0.67 - ETA: 10:39 - loss: 5.2652 - acc: 0.67 - ETA: 9:20 - loss: 5.2074 - acc: 0.6769 - ETA: 8:00 - loss: 5.1348 - acc: 0.681 - ETA: 6:40 - loss: 5.1470 - acc: 0.680 - ETA: 5:19 - loss: 5.1376 - acc: 0.681 - ETA: 3:59 - loss: 5.0914 - acc: 0.684 - ETA: 2:40 - loss: 5.0772 - acc: 0.685 - ETA: 1:20 - loss: 5.0136 - acc: 0.688 - 1638s 819ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 7.73669\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 25:29 - loss: 5.1578 - acc: 0.68 - ETA: 24:12 - loss: 5.0772 - acc: 0.68 - ETA: 22:56 - loss: 4.8892 - acc: 0.69 - ETA: 21:42 - loss: 4.7145 - acc: 0.70 - ETA: 20:17 - loss: 4.6098 - acc: 0.71 - ETA: 18:59 - loss: 4.4862 - acc: 0.72 - ETA: 17:40 - loss: 4.4210 - acc: 0.72 - ETA: 16:14 - loss: 4.5534 - acc: 0.71 - ETA: 14:51 - loss: 4.6742 - acc: 0.71 - ETA: 13:28 - loss: 4.7548 - acc: 0.70 - ETA: 12:08 - loss: 4.7768 - acc: 0.70 - ETA: 10:46 - loss: 4.7951 - acc: 0.70 - ETA: 9:25 - loss: 4.8478 - acc: 0.6992 - ETA: 8:03 - loss: 4.9506 - acc: 0.692 - ETA: 6:43 - loss: 5.0074 - acc: 0.689 - ETA: 5:23 - loss: 5.0369 - acc: 0.687 - ETA: 4:02 - loss: 4.9776 - acc: 0.691 - ETA: 2:41 - loss: 5.0056 - acc: 0.689 - ETA: 1:20 - loss: 5.0645 - acc: 0.685 - 1647s 824ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 7.73669\n",
      "Epoch 34/40\n",
      "2000/2000 [==============================] - ETA: 25:42 - loss: 4.0295 - acc: 0.75 - ETA: 24:19 - loss: 5.5607 - acc: 0.65 - ETA: 22:46 - loss: 5.5876 - acc: 0.65 - ETA: 21:35 - loss: 5.3996 - acc: 0.66 - ETA: 20:15 - loss: 5.1900 - acc: 0.67 - ETA: 18:50 - loss: 5.1578 - acc: 0.68 - ETA: 17:27 - loss: 5.0657 - acc: 0.68 - ETA: 16:06 - loss: 5.0369 - acc: 0.68 - ETA: 14:45 - loss: 4.9966 - acc: 0.69 - ETA: 13:24 - loss: 4.9966 - acc: 0.69 - ETA: 12:03 - loss: 4.8794 - acc: 0.69 - ETA: 10:45 - loss: 4.9160 - acc: 0.69 - ETA: 9:24 - loss: 5.0214 - acc: 0.6885 - ETA: 8:04 - loss: 5.0311 - acc: 0.687 - ETA: 6:43 - loss: 5.0396 - acc: 0.687 - ETA: 5:23 - loss: 5.0772 - acc: 0.685 - ETA: 4:02 - loss: 5.0251 - acc: 0.688 - ETA: 2:41 - loss: 5.0324 - acc: 0.687 - ETA: 1:20 - loss: 5.0899 - acc: 0.684 - 1652s 826ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 7.73669\n",
      "Epoch 35/40\n",
      "2000/2000 [==============================] - ETA: 25:18 - loss: 4.1907 - acc: 0.74 - ETA: 23:59 - loss: 4.6742 - acc: 0.71 - ETA: 22:57 - loss: 4.7280 - acc: 0.70 - ETA: 21:38 - loss: 4.7951 - acc: 0.70 - ETA: 20:13 - loss: 4.8354 - acc: 0.70 - ETA: 18:49 - loss: 5.1309 - acc: 0.68 - ETA: 17:27 - loss: 5.3190 - acc: 0.67 - ETA: 16:06 - loss: 5.3190 - acc: 0.67 - ETA: 14:45 - loss: 5.3369 - acc: 0.66 - ETA: 13:25 - loss: 5.3029 - acc: 0.67 - ETA: 12:05 - loss: 5.2311 - acc: 0.67 - ETA: 10:44 - loss: 5.1041 - acc: 0.68 - ETA: 9:23 - loss: 5.0958 - acc: 0.6838 - ETA: 8:04 - loss: 5.0772 - acc: 0.685 - ETA: 6:44 - loss: 5.0826 - acc: 0.684 - ETA: 5:23 - loss: 5.1578 - acc: 0.680 - ETA: 4:02 - loss: 5.1009 - acc: 0.683 - ETA: 2:41 - loss: 5.0862 - acc: 0.684 - ETA: 1:20 - loss: 5.0814 - acc: 0.684 - 1653s 826ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 7.73669\n",
      "Epoch 36/40\n",
      "2000/2000 [==============================] - ETA: 25:18 - loss: 6.1249 - acc: 0.62 - ETA: 24:02 - loss: 5.1578 - acc: 0.68 - ETA: 22:46 - loss: 5.1578 - acc: 0.68 - ETA: 21:29 - loss: 4.9160 - acc: 0.69 - ETA: 20:12 - loss: 4.9321 - acc: 0.69 - ETA: 18:47 - loss: 4.9160 - acc: 0.69 - ETA: 17:32 - loss: 4.9736 - acc: 0.69 - ETA: 16:11 - loss: 5.0973 - acc: 0.68 - ETA: 14:51 - loss: 5.1936 - acc: 0.67 - ETA: 13:31 - loss: 5.2384 - acc: 0.67 - ETA: 12:09 - loss: 5.2750 - acc: 0.67 - ETA: 10:49 - loss: 5.2518 - acc: 0.67 - ETA: 9:28 - loss: 5.1950 - acc: 0.6777 - ETA: 8:07 - loss: 5.0311 - acc: 0.687 - ETA: 6:45 - loss: 5.0611 - acc: 0.686 - ETA: 5:24 - loss: 5.0369 - acc: 0.687 - ETA: 4:02 - loss: 5.0345 - acc: 0.687 - ETA: 2:41 - loss: 5.0503 - acc: 0.686 - ETA: 1:20 - loss: 5.0390 - acc: 0.687 - 1654s 827ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 7.73669\n",
      "Epoch 37/40\n",
      "2000/2000 [==============================] - ETA: 25:18 - loss: 4.6742 - acc: 0.71 - ETA: 24:12 - loss: 4.8354 - acc: 0.70 - ETA: 22:45 - loss: 5.1578 - acc: 0.68 - ETA: 21:20 - loss: 5.2384 - acc: 0.67 - ETA: 20:10 - loss: 5.2867 - acc: 0.67 - ETA: 18:47 - loss: 5.3458 - acc: 0.66 - ETA: 17:25 - loss: 5.1808 - acc: 0.67 - ETA: 16:04 - loss: 5.2182 - acc: 0.67 - ETA: 14:44 - loss: 5.2832 - acc: 0.67 - ETA: 13:26 - loss: 5.2223 - acc: 0.67 - ETA: 12:06 - loss: 5.2457 - acc: 0.67 - ETA: 10:45 - loss: 5.1847 - acc: 0.67 - ETA: 9:25 - loss: 5.1950 - acc: 0.6777 - ETA: 8:05 - loss: 5.2154 - acc: 0.676 - ETA: 6:44 - loss: 5.2545 - acc: 0.674 - ETA: 5:23 - loss: 5.1880 - acc: 0.678 - ETA: 4:03 - loss: 5.1199 - acc: 0.682 - ETA: 2:41 - loss: 5.0951 - acc: 0.683 - ETA: 1:20 - loss: 5.0645 - acc: 0.685 - 1652s 826ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 7.73669\n",
      "Epoch 38/40\n",
      "2000/2000 [==============================] - ETA: 25:15 - loss: 5.4802 - acc: 0.66 - ETA: 23:50 - loss: 5.3996 - acc: 0.66 - ETA: 22:21 - loss: 5.2115 - acc: 0.67 - ETA: 21:19 - loss: 5.2384 - acc: 0.67 - ETA: 19:56 - loss: 5.3190 - acc: 0.67 - ETA: 18:38 - loss: 5.2652 - acc: 0.67 - ETA: 17:20 - loss: 5.1348 - acc: 0.68 - ETA: 16:06 - loss: 5.1175 - acc: 0.68 - ETA: 14:44 - loss: 5.1041 - acc: 0.68 - ETA: 13:27 - loss: 5.0611 - acc: 0.68 - ETA: 12:07 - loss: 5.0406 - acc: 0.68 - ETA: 10:45 - loss: 5.0369 - acc: 0.68 - ETA: 9:25 - loss: 5.1206 - acc: 0.6823 - ETA: 8:03 - loss: 5.0772 - acc: 0.685 - ETA: 6:42 - loss: 5.0933 - acc: 0.684 - ETA: 5:21 - loss: 5.1276 - acc: 0.681 - ETA: 4:01 - loss: 5.1199 - acc: 0.682 - ETA: 2:40 - loss: 5.0503 - acc: 0.686 - ETA: 1:20 - loss: 5.0221 - acc: 0.688 - 1645s 822ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 7.73669\n",
      "Epoch 39/40\n",
      "2000/2000 [==============================] - ETA: 25:04 - loss: 5.6413 - acc: 0.65 - ETA: 23:37 - loss: 5.4802 - acc: 0.66 - ETA: 22:20 - loss: 5.3727 - acc: 0.66 - ETA: 21:12 - loss: 5.2787 - acc: 0.67 - ETA: 19:55 - loss: 5.1256 - acc: 0.68 - ETA: 18:34 - loss: 5.0503 - acc: 0.68 - ETA: 17:14 - loss: 5.1348 - acc: 0.68 - ETA: 15:52 - loss: 4.9966 - acc: 0.69 - ETA: 14:37 - loss: 4.8892 - acc: 0.69 - ETA: 13:20 - loss: 5.0611 - acc: 0.68 - ETA: 12:01 - loss: 5.0699 - acc: 0.68 - ETA: 10:43 - loss: 5.0235 - acc: 0.68 - ETA: 9:22 - loss: 5.1330 - acc: 0.6815 - ETA: 8:01 - loss: 5.1348 - acc: 0.681 - ETA: 6:42 - loss: 5.1363 - acc: 0.681 - ETA: 5:21 - loss: 5.1779 - acc: 0.678 - ETA: 4:00 - loss: 5.1388 - acc: 0.681 - ETA: 2:40 - loss: 5.1130 - acc: 0.682 - ETA: 1:20 - loss: 5.1069 - acc: 0.683 - 1638s 819ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 7.73669\n",
      "Epoch 40/40\n",
      "2000/2000 [==============================] - ETA: 25:07 - loss: 5.3190 - acc: 0.67 - ETA: 23:46 - loss: 4.5131 - acc: 0.72 - ETA: 22:30 - loss: 4.6742 - acc: 0.71 - ETA: 21:12 - loss: 4.6742 - acc: 0.71 - ETA: 20:00 - loss: 4.8032 - acc: 0.70 - ETA: 18:41 - loss: 5.1041 - acc: 0.68 - ETA: 17:22 - loss: 4.9045 - acc: 0.69 - ETA: 16:01 - loss: 5.1376 - acc: 0.68 - ETA: 14:40 - loss: 5.1220 - acc: 0.68 - ETA: 13:25 - loss: 5.1417 - acc: 0.68 - ETA: 12:08 - loss: 5.1138 - acc: 0.68 - ETA: 10:49 - loss: 5.1712 - acc: 0.67 - ETA: 9:27 - loss: 5.1826 - acc: 0.6785 - ETA: 8:05 - loss: 5.0657 - acc: 0.685 - ETA: 6:46 - loss: 5.0503 - acc: 0.686 - ETA: 5:24 - loss: 5.0470 - acc: 0.686 - ETA: 4:03 - loss: 5.0535 - acc: 0.686 - ETA: 2:42 - loss: 5.1041 - acc: 0.683 - ETA: 1:21 - loss: 5.0899 - acc: 0.684 - 1655s 827ms/step - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 7.73669\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "\n",
    "checkpoint_filepath = 'saved_models/weights.best.my.hdf5'\n",
    "\n",
    "my_checkpointer = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "my_model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=40, batch_size=100, callbacks=[my_checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1185.5332182366174\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "my_model.load_weights(checkpoint_filepath)\n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1185.5665660679726\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "my_predictions = [my_model.predict(np.expand_dims(feature, axis=0)) for feature in test_tensors]\n",
    "\n",
    "# test_accuracy = 100 * np.sum(np.array(my_predictions)==np.argmax(test_targets, axis=1)) / len(my_predictions)\n",
    "# print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "\n",
    "with open('my_cnn.csv', 'w', newline='') as csvfile:\n",
    "    result_writger = csv.writer(csvfile)\n",
    "    result_writger.writerow(['Id', 'task_1', 'task_2'])\n",
    "    for test_filepath, test_prediction in zip(test_files, my_predictions):\n",
    "        result_writger.writerow([test_filepath, test_prediction[0][0], test_prediction[0][2]])\n",
    "        \n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
