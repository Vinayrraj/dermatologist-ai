{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "def print_elapsed_time():\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "def start_timer():\n",
    "    elapsed = (timeit.default_timer() - start_time)/60\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.838409904550645e-06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files   \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(data_path, shuffle=None):\n",
    "    kwargs = {}\n",
    "    if shuffle != None:\n",
    "        kwargs['shuffle'] = shuffle\n",
    "    data = load_files(data_path, **kwargs)\n",
    "    img_files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), 3)\n",
    "    return img_files, targets\n",
    "\n",
    "\n",
    "\n",
    "start_timer()\n",
    "train_files, train_targets = load_dataset('data/train')\n",
    "valid_files, valid_targets = load_dataset('data/valid')\n",
    "test_files, test_targets = load_dataset('data/test', shuffle=False)\n",
    "\n",
    "# load lables\n",
    "label_name = [item[11:-1] for item in sorted(glob(\"data/train/*/\"))]\n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.641631641528268\n",
      "train_files size: 2000\n",
      "train_files shape: (2000,)\n",
      "target shape: (2000, 3)\n",
      "['melanoma', 'nevus', 'seborrheic_keratosis']\n",
      "9.130685384661774e-07\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "print('train_files size: {}'.format(len(train_files)))\n",
    "print('train_files shape: {}'.format(train_files.shape))\n",
    "print('target shape: {}'.format(train_targets.shape))\n",
    "print(label_name)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "elapsed = (timeit.default_timer() - start_time)/60\n",
    "print(elapsed)\n",
    "\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010503104177056837\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size=(384, 256))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(image_paths):\n",
    "    return np.vstack([path_to_tensor(path) for path in image_paths])\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning and putting images into tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014982516017501741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [07:03<00:00,  4.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:58<00:00,  2.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [05:40<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 384, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "train_tensors = paths_to_tensor(tqdm(train_files))\n",
    "valid_tensors = paths_to_tensor(tqdm(valid_files))\n",
    "test_tensors = paths_to_tensor(tqdm(test_files))\n",
    "\n",
    "print(train_tensors.shape)\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flip Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.792365996098898\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "apply_train_image_transform = False\n",
    "\n",
    "if apply_train_image_transform:\n",
    "    # Caution: Doesn't guarantee prevention of duplication.\n",
    "    datagen_train = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "    \n",
    "    datagen_train.fit(train_tensors)\n",
    "    shape = (train_tensors.shape[0] * 2,) + train_tensors.shape[1:]\n",
    "    generated = np.ndarray(shape=shape)\n",
    "    for i, image in tqdm(enumerate(train_tensors)):\n",
    "        generated[i] = datagen_train.random_transform(image)\n",
    "    \n",
    "    train_tensors = np.concatenate((train_tensors, generated))\n",
    "    train_targets = train_targets.repeat(2, axis=0)\n",
    "    \n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning using Inception Resnet V2¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.793053152706186\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "train_imgs_preprocess = preprocess_input(train_tensors)\n",
    "valid_imgs_preprocess = preprocess_input(valid_tensors)\n",
    "test_imgs_preprocess = preprocess_input(test_tensors)\n",
    "del train_tensors, valid_tensors, test_tensors\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.832828193098809\n",
      "(2000, 10, 6, 1536)\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "transfer_model = InceptionResNetV2(include_top=False)\n",
    "\n",
    "train_data = transfer_model.predict(train_imgs_preprocess)\n",
    "valid_data = transfer_model.predict(valid_imgs_preprocess)\n",
    "test_data = transfer_model.predict(test_imgs_preprocess)\n",
    "\n",
    "del train_imgs_preprocess, valid_imgs_preprocess, test_imgs_preprocess\n",
    "print(train_data.shape)\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.63998663335929\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 1,576,963\n",
      "Trainable params: 1,576,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "from keras.layers import Conv2D, Dropout, Flatten, Dense, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "my_model = Sequential()\n",
    "\n",
    "my_model.add(GlobalAveragePooling2D(input_shape=train_data.shape[1:]))\n",
    "my_model.add(Dropout(0.2))\n",
    "my_model.add(Dense(1024, activation='relu'))\n",
    "my_model.add(Dropout(0.2))\n",
    "my_model.add(Dense(3, activation='softmax'))\n",
    "my_model.summary()\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.64254146739158\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.64438656557036\n",
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/70\n",
      "2000/2000 [==============================] - ETA: 1:04 - loss: 1.5355 - acc: 0.155 - ETA: 29s - loss: 2.7951 - acc: 0.420 - ETA: 17s - loss: 3.3344 - acc: 0.50 - ETA: 11s - loss: 3.4588 - acc: 0.52 - ETA: 8s - loss: 3.0836 - acc: 0.5690 - ETA: 5s - loss: 2.7448 - acc: 0.584 - ETA: 3s - loss: 2.4783 - acc: 0.585 - ETA: 2s - loss: 2.3434 - acc: 0.536 - ETA: 0s - loss: 2.2224 - acc: 0.505 - 10s 5ms/step - loss: 2.1114 - acc: 0.4845 - val_loss: 0.9932 - val_acc: 0.5133\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.99318, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 2/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.9658 - acc: 0.485 - ETA: 1s - loss: 0.8843 - acc: 0.565 - ETA: 1s - loss: 0.8492 - acc: 0.606 - ETA: 0s - loss: 0.8372 - acc: 0.630 - ETA: 0s - loss: 0.8285 - acc: 0.645 - ETA: 0s - loss: 0.8427 - acc: 0.649 - ETA: 0s - loss: 0.8328 - acc: 0.657 - ETA: 0s - loss: 0.8443 - acc: 0.657 - ETA: 0s - loss: 0.8312 - acc: 0.665 - 2s 773us/step - loss: 0.8312 - acc: 0.6670 - val_loss: 1.0074 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.99318\n",
      "Epoch 3/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.7214 - acc: 0.705 - ETA: 1s - loss: 0.7396 - acc: 0.682 - ETA: 1s - loss: 0.7412 - acc: 0.688 - ETA: 0s - loss: 0.7437 - acc: 0.695 - ETA: 0s - loss: 0.7440 - acc: 0.693 - ETA: 0s - loss: 0.7385 - acc: 0.696 - ETA: 0s - loss: 0.7399 - acc: 0.699 - ETA: 0s - loss: 0.7479 - acc: 0.691 - ETA: 0s - loss: 0.7468 - acc: 0.691 - 2s 762us/step - loss: 0.7453 - acc: 0.6920 - val_loss: 0.9002 - val_acc: 0.5867\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.99318 to 0.90020, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 4/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.7432 - acc: 0.680 - ETA: 1s - loss: 0.7171 - acc: 0.695 - ETA: 1s - loss: 0.7058 - acc: 0.700 - ETA: 0s - loss: 0.7080 - acc: 0.700 - ETA: 0s - loss: 0.6974 - acc: 0.713 - ETA: 0s - loss: 0.7154 - acc: 0.702 - ETA: 0s - loss: 0.7232 - acc: 0.696 - ETA: 0s - loss: 0.7283 - acc: 0.693 - ETA: 0s - loss: 0.7198 - acc: 0.696 - 2s 773us/step - loss: 0.7155 - acc: 0.6975 - val_loss: 0.8882 - val_acc: 0.5733\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.90020 to 0.88822, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 5/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.7408 - acc: 0.715 - ETA: 1s - loss: 0.7048 - acc: 0.707 - ETA: 1s - loss: 0.7022 - acc: 0.705 - ETA: 0s - loss: 0.7017 - acc: 0.705 - ETA: 0s - loss: 0.7042 - acc: 0.701 - ETA: 0s - loss: 0.6983 - acc: 0.696 - ETA: 0s - loss: 0.6994 - acc: 0.700 - ETA: 0s - loss: 0.6980 - acc: 0.701 - ETA: 0s - loss: 0.6956 - acc: 0.704 - 2s 852us/step - loss: 0.6890 - acc: 0.7060 - val_loss: 0.8848 - val_acc: 0.5733\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.88822 to 0.88480, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 6/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.7088 - acc: 0.645 - ETA: 1s - loss: 0.6956 - acc: 0.687 - ETA: 1s - loss: 0.6883 - acc: 0.688 - ETA: 0s - loss: 0.6799 - acc: 0.698 - ETA: 0s - loss: 0.6619 - acc: 0.709 - ETA: 0s - loss: 0.6565 - acc: 0.711 - ETA: 0s - loss: 0.6594 - acc: 0.709 - ETA: 0s - loss: 0.6632 - acc: 0.706 - ETA: 0s - loss: 0.6657 - acc: 0.707 - 2s 805us/step - loss: 0.6670 - acc: 0.7075 - val_loss: 0.8382 - val_acc: 0.5867\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.88480 to 0.83818, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 7/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6979 - acc: 0.670 - ETA: 1s - loss: 0.6796 - acc: 0.680 - ETA: 1s - loss: 0.6736 - acc: 0.696 - ETA: 0s - loss: 0.6586 - acc: 0.711 - ETA: 0s - loss: 0.6722 - acc: 0.702 - ETA: 0s - loss: 0.6584 - acc: 0.709 - ETA: 0s - loss: 0.6605 - acc: 0.710 - ETA: 0s - loss: 0.6676 - acc: 0.710 - ETA: 0s - loss: 0.6668 - acc: 0.715 - 2s 766us/step - loss: 0.6624 - acc: 0.7155 - val_loss: 0.8460 - val_acc: 0.5733\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.83818\n",
      "Epoch 8/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6247 - acc: 0.725 - ETA: 1s - loss: 0.6533 - acc: 0.715 - ETA: 1s - loss: 0.6684 - acc: 0.705 - ETA: 0s - loss: 0.6607 - acc: 0.710 - ETA: 0s - loss: 0.6449 - acc: 0.715 - ETA: 0s - loss: 0.6323 - acc: 0.720 - ETA: 0s - loss: 0.6250 - acc: 0.729 - ETA: 0s - loss: 0.6238 - acc: 0.733 - ETA: 0s - loss: 0.6350 - acc: 0.729 - 2s 770us/step - loss: 0.6407 - acc: 0.7250 - val_loss: 0.7847 - val_acc: 0.6200\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.83818 to 0.78468, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 9/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5625 - acc: 0.750 - ETA: 1s - loss: 0.6175 - acc: 0.727 - ETA: 1s - loss: 0.6254 - acc: 0.730 - ETA: 0s - loss: 0.6376 - acc: 0.730 - ETA: 0s - loss: 0.6390 - acc: 0.730 - ETA: 0s - loss: 0.6400 - acc: 0.731 - ETA: 0s - loss: 0.6319 - acc: 0.737 - ETA: 0s - loss: 0.6331 - acc: 0.736 - ETA: 0s - loss: 0.6307 - acc: 0.737 - 2s 766us/step - loss: 0.6312 - acc: 0.7340 - val_loss: 0.8149 - val_acc: 0.6067\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.78468\n",
      "Epoch 10/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6112 - acc: 0.725 - ETA: 1s - loss: 0.6339 - acc: 0.720 - ETA: 1s - loss: 0.6499 - acc: 0.708 - ETA: 0s - loss: 0.6352 - acc: 0.717 - ETA: 0s - loss: 0.6294 - acc: 0.722 - ETA: 0s - loss: 0.6241 - acc: 0.725 - ETA: 0s - loss: 0.6206 - acc: 0.727 - ETA: 0s - loss: 0.6224 - acc: 0.730 - ETA: 0s - loss: 0.6090 - acc: 0.738 - 2s 764us/step - loss: 0.6175 - acc: 0.7355 - val_loss: 0.7863 - val_acc: 0.6467\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.78468\n",
      "Epoch 11/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6208 - acc: 0.750 - ETA: 1s - loss: 0.6364 - acc: 0.725 - ETA: 0s - loss: 0.6591 - acc: 0.711 - ETA: 0s - loss: 0.6320 - acc: 0.732 - ETA: 0s - loss: 0.6236 - acc: 0.729 - ETA: 0s - loss: 0.6185 - acc: 0.731 - ETA: 0s - loss: 0.6110 - acc: 0.732 - ETA: 0s - loss: 0.6028 - acc: 0.739 - ETA: 0s - loss: 0.6035 - acc: 0.738 - 2s 757us/step - loss: 0.6076 - acc: 0.7385 - val_loss: 0.7693 - val_acc: 0.6400\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.78468 to 0.76934, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 12/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.6451 - acc: 0.710 - ETA: 1s - loss: 0.5963 - acc: 0.730 - ETA: 1s - loss: 0.5708 - acc: 0.748 - ETA: 0s - loss: 0.5985 - acc: 0.733 - ETA: 0s - loss: 0.6018 - acc: 0.734 - ETA: 0s - loss: 0.5912 - acc: 0.744 - ETA: 0s - loss: 0.5943 - acc: 0.741 - ETA: 0s - loss: 0.5913 - acc: 0.744 - ETA: 0s - loss: 0.5839 - acc: 0.749 - 2s 773us/step - loss: 0.5866 - acc: 0.7500 - val_loss: 0.7843 - val_acc: 0.6400\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.76934\n",
      "Epoch 13/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5726 - acc: 0.735 - ETA: 1s - loss: 0.6102 - acc: 0.705 - ETA: 0s - loss: 0.5933 - acc: 0.720 - ETA: 0s - loss: 0.5886 - acc: 0.732 - ETA: 0s - loss: 0.5879 - acc: 0.739 - ETA: 0s - loss: 0.5760 - acc: 0.755 - ETA: 0s - loss: 0.5871 - acc: 0.748 - ETA: 0s - loss: 0.5856 - acc: 0.752 - ETA: 0s - loss: 0.5948 - acc: 0.746 - 2s 760us/step - loss: 0.5960 - acc: 0.7440 - val_loss: 0.7481 - val_acc: 0.6733\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.76934 to 0.74808, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 14/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5914 - acc: 0.760 - ETA: 1s - loss: 0.5618 - acc: 0.767 - ETA: 1s - loss: 0.5524 - acc: 0.776 - ETA: 0s - loss: 0.5488 - acc: 0.776 - ETA: 0s - loss: 0.5658 - acc: 0.770 - ETA: 0s - loss: 0.5699 - acc: 0.767 - ETA: 0s - loss: 0.5729 - acc: 0.762 - ETA: 0s - loss: 0.5783 - acc: 0.763 - ETA: 0s - loss: 0.5773 - acc: 0.761 - 2s 766us/step - loss: 0.5731 - acc: 0.7615 - val_loss: 0.7225 - val_acc: 0.6733\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.74808 to 0.72248, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 15/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5522 - acc: 0.825 - ETA: 1s - loss: 0.5875 - acc: 0.772 - ETA: 1s - loss: 0.5949 - acc: 0.768 - ETA: 0s - loss: 0.6026 - acc: 0.756 - ETA: 0s - loss: 0.5869 - acc: 0.755 - ETA: 0s - loss: 0.5721 - acc: 0.762 - ETA: 0s - loss: 0.5731 - acc: 0.758 - ETA: 0s - loss: 0.5774 - acc: 0.756 - ETA: 0s - loss: 0.5680 - acc: 0.763 - 2s 773us/step - loss: 0.5693 - acc: 0.7645 - val_loss: 0.7322 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.72248\n",
      "Epoch 16/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5536 - acc: 0.785 - ETA: 1s - loss: 0.5412 - acc: 0.790 - ETA: 1s - loss: 0.5841 - acc: 0.760 - ETA: 0s - loss: 0.6198 - acc: 0.736 - ETA: 0s - loss: 0.5950 - acc: 0.748 - ETA: 0s - loss: 0.5885 - acc: 0.755 - ETA: 0s - loss: 0.5913 - acc: 0.752 - ETA: 0s - loss: 0.5904 - acc: 0.752 - ETA: 0s - loss: 0.5784 - acc: 0.762 - 2s 754us/step - loss: 0.5750 - acc: 0.7605 - val_loss: 0.8023 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.72248\n",
      "Epoch 17/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5517 - acc: 0.765 - ETA: 1s - loss: 0.5288 - acc: 0.775 - ETA: 1s - loss: 0.5443 - acc: 0.750 - ETA: 0s - loss: 0.5227 - acc: 0.763 - ETA: 0s - loss: 0.5212 - acc: 0.770 - ETA: 0s - loss: 0.5235 - acc: 0.774 - ETA: 0s - loss: 0.5323 - acc: 0.772 - ETA: 0s - loss: 0.5404 - acc: 0.765 - ETA: 0s - loss: 0.5419 - acc: 0.763 - 2s 763us/step - loss: 0.5444 - acc: 0.7620 - val_loss: 0.7132 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.72248 to 0.71319, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 18/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5674 - acc: 0.790 - ETA: 1s - loss: 0.5412 - acc: 0.797 - ETA: 1s - loss: 0.5411 - acc: 0.796 - ETA: 0s - loss: 0.5427 - acc: 0.790 - ETA: 0s - loss: 0.5398 - acc: 0.786 - ETA: 0s - loss: 0.5413 - acc: 0.784 - ETA: 0s - loss: 0.5510 - acc: 0.775 - ETA: 0s - loss: 0.5506 - acc: 0.773 - ETA: 0s - loss: 0.5507 - acc: 0.775 - 2s 766us/step - loss: 0.5486 - acc: 0.7760 - val_loss: 0.7041 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.71319 to 0.70414, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 19/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5837 - acc: 0.720 - ETA: 1s - loss: 0.5608 - acc: 0.755 - ETA: 1s - loss: 0.5511 - acc: 0.763 - ETA: 0s - loss: 0.5398 - acc: 0.767 - ETA: 0s - loss: 0.5425 - acc: 0.766 - ETA: 0s - loss: 0.5594 - acc: 0.757 - ETA: 0s - loss: 0.5507 - acc: 0.765 - ETA: 0s - loss: 0.5557 - acc: 0.763 - ETA: 0s - loss: 0.5597 - acc: 0.760 - 2s 773us/step - loss: 0.5622 - acc: 0.7570 - val_loss: 0.6883 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.70414 to 0.68834, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 20/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5118 - acc: 0.825 - ETA: 1s - loss: 0.5143 - acc: 0.792 - ETA: 1s - loss: 0.5513 - acc: 0.768 - ETA: 0s - loss: 0.5664 - acc: 0.753 - ETA: 0s - loss: 0.5663 - acc: 0.751 - ETA: 0s - loss: 0.5595 - acc: 0.759 - ETA: 0s - loss: 0.5525 - acc: 0.763 - ETA: 0s - loss: 0.5547 - acc: 0.764 - ETA: 0s - loss: 0.5465 - acc: 0.770 - 2s 766us/step - loss: 0.5466 - acc: 0.7710 - val_loss: 0.7662 - val_acc: 0.6600\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.68834\n",
      "Epoch 21/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5520 - acc: 0.770 - ETA: 1s - loss: 0.5670 - acc: 0.747 - ETA: 1s - loss: 0.5328 - acc: 0.770 - ETA: 0s - loss: 0.5346 - acc: 0.772 - ETA: 0s - loss: 0.5515 - acc: 0.774 - ETA: 0s - loss: 0.5528 - acc: 0.765 - ETA: 0s - loss: 0.5442 - acc: 0.774 - ETA: 0s - loss: 0.5421 - acc: 0.773 - ETA: 0s - loss: 0.5529 - acc: 0.767 - 2s 779us/step - loss: 0.5496 - acc: 0.7705 - val_loss: 0.7135 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.68834\n",
      "Epoch 22/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4618 - acc: 0.810 - ETA: 1s - loss: 0.4954 - acc: 0.792 - ETA: 1s - loss: 0.5399 - acc: 0.778 - ETA: 0s - loss: 0.5279 - acc: 0.778 - ETA: 0s - loss: 0.5305 - acc: 0.784 - ETA: 0s - loss: 0.5268 - acc: 0.784 - ETA: 0s - loss: 0.5345 - acc: 0.777 - ETA: 0s - loss: 0.5373 - acc: 0.778 - ETA: 0s - loss: 0.5362 - acc: 0.780 - 2s 770us/step - loss: 0.5289 - acc: 0.7855 - val_loss: 0.7372 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.68834\n",
      "Epoch 23/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5120 - acc: 0.760 - ETA: 1s - loss: 0.4976 - acc: 0.787 - ETA: 0s - loss: 0.5026 - acc: 0.781 - ETA: 0s - loss: 0.5127 - acc: 0.776 - ETA: 0s - loss: 0.5094 - acc: 0.774 - ETA: 0s - loss: 0.5054 - acc: 0.777 - ETA: 0s - loss: 0.5216 - acc: 0.768 - ETA: 0s - loss: 0.5173 - acc: 0.775 - ETA: 0s - loss: 0.5189 - acc: 0.774 - 2s 767us/step - loss: 0.5211 - acc: 0.7740 - val_loss: 0.7077 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.68834\n",
      "Epoch 24/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.5399 - acc: 0.800 - ETA: 1s - loss: 0.5498 - acc: 0.772 - ETA: 1s - loss: 0.5395 - acc: 0.773 - ETA: 0s - loss: 0.5312 - acc: 0.775 - ETA: 0s - loss: 0.5200 - acc: 0.785 - ETA: 0s - loss: 0.5154 - acc: 0.785 - ETA: 0s - loss: 0.5150 - acc: 0.786 - ETA: 0s - loss: 0.5087 - acc: 0.790 - ETA: 0s - loss: 0.5070 - acc: 0.791 - 2s 774us/step - loss: 0.5030 - acc: 0.7920 - val_loss: 0.6984 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.68834\n",
      "Epoch 25/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4838 - acc: 0.840 - ETA: 1s - loss: 0.5025 - acc: 0.812 - ETA: 0s - loss: 0.5010 - acc: 0.811 - ETA: 0s - loss: 0.5065 - acc: 0.807 - ETA: 0s - loss: 0.5065 - acc: 0.803 - ETA: 0s - loss: 0.5069 - acc: 0.797 - ETA: 0s - loss: 0.5074 - acc: 0.796 - ETA: 0s - loss: 0.5028 - acc: 0.797 - ETA: 0s - loss: 0.5025 - acc: 0.793 - 2s 768us/step - loss: 0.5027 - acc: 0.7945 - val_loss: 0.7069 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.68834\n",
      "Epoch 26/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4964 - acc: 0.795 - ETA: 1s - loss: 0.4977 - acc: 0.805 - ETA: 1s - loss: 0.4931 - acc: 0.803 - ETA: 0s - loss: 0.5172 - acc: 0.797 - ETA: 0s - loss: 0.5154 - acc: 0.795 - ETA: 0s - loss: 0.5142 - acc: 0.793 - ETA: 0s - loss: 0.5185 - acc: 0.789 - ETA: 0s - loss: 0.5140 - acc: 0.791 - ETA: 0s - loss: 0.5124 - acc: 0.789 - 2s 779us/step - loss: 0.5098 - acc: 0.7920 - val_loss: 0.7061 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.68834\n",
      "Epoch 27/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4937 - acc: 0.790 - ETA: 1s - loss: 0.4926 - acc: 0.797 - ETA: 1s - loss: 0.4880 - acc: 0.790 - ETA: 0s - loss: 0.4914 - acc: 0.793 - ETA: 0s - loss: 0.5017 - acc: 0.790 - ETA: 0s - loss: 0.4957 - acc: 0.791 - ETA: 0s - loss: 0.4904 - acc: 0.792 - ETA: 0s - loss: 0.4994 - acc: 0.785 - ETA: 0s - loss: 0.4975 - acc: 0.789 - 2s 777us/step - loss: 0.4982 - acc: 0.7870 - val_loss: 0.6788 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.68834 to 0.67879, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 28/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4624 - acc: 0.820 - ETA: 1s - loss: 0.4975 - acc: 0.810 - ETA: 1s - loss: 0.4631 - acc: 0.821 - ETA: 0s - loss: 0.4623 - acc: 0.815 - ETA: 0s - loss: 0.4685 - acc: 0.809 - ETA: 0s - loss: 0.4827 - acc: 0.793 - ETA: 0s - loss: 0.4668 - acc: 0.802 - ETA: 0s - loss: 0.4822 - acc: 0.799 - ETA: 0s - loss: 0.4917 - acc: 0.794 - 2s 773us/step - loss: 0.4945 - acc: 0.7940 - val_loss: 0.6807 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.67879\n",
      "Epoch 29/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4009 - acc: 0.860 - ETA: 1s - loss: 0.4345 - acc: 0.842 - ETA: 1s - loss: 0.4756 - acc: 0.816 - ETA: 0s - loss: 0.4908 - acc: 0.807 - ETA: 0s - loss: 0.4780 - acc: 0.813 - ETA: 0s - loss: 0.4745 - acc: 0.809 - ETA: 0s - loss: 0.4799 - acc: 0.807 - ETA: 0s - loss: 0.4835 - acc: 0.806 - ETA: 0s - loss: 0.4851 - acc: 0.808 - 2s 781us/step - loss: 0.4840 - acc: 0.8095 - val_loss: 0.6689 - val_acc: 0.7467\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00029: val_loss improved from 0.67879 to 0.66892, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 30/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4953 - acc: 0.805 - ETA: 1s - loss: 0.4697 - acc: 0.817 - ETA: 0s - loss: 0.4844 - acc: 0.815 - ETA: 0s - loss: 0.4906 - acc: 0.803 - ETA: 0s - loss: 0.4842 - acc: 0.803 - ETA: 0s - loss: 0.4792 - acc: 0.807 - ETA: 0s - loss: 0.4786 - acc: 0.803 - ETA: 0s - loss: 0.4787 - acc: 0.802 - ETA: 0s - loss: 0.4745 - acc: 0.803 - 2s 766us/step - loss: 0.4725 - acc: 0.8050 - val_loss: 0.7247 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.66892\n",
      "Epoch 31/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4286 - acc: 0.815 - ETA: 1s - loss: 0.4359 - acc: 0.815 - ETA: 0s - loss: 0.4500 - acc: 0.805 - ETA: 0s - loss: 0.4462 - acc: 0.813 - ETA: 0s - loss: 0.4559 - acc: 0.808 - ETA: 0s - loss: 0.4579 - acc: 0.812 - ETA: 0s - loss: 0.4567 - acc: 0.813 - ETA: 0s - loss: 0.4549 - acc: 0.812 - ETA: 0s - loss: 0.4665 - acc: 0.807 - 2s 769us/step - loss: 0.4640 - acc: 0.8100 - val_loss: 0.6699 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.66892\n",
      "Epoch 32/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4306 - acc: 0.815 - ETA: 1s - loss: 0.4363 - acc: 0.815 - ETA: 1s - loss: 0.4582 - acc: 0.798 - ETA: 0s - loss: 0.4771 - acc: 0.791 - ETA: 0s - loss: 0.4634 - acc: 0.804 - ETA: 0s - loss: 0.4617 - acc: 0.802 - ETA: 0s - loss: 0.4687 - acc: 0.797 - ETA: 0s - loss: 0.4666 - acc: 0.800 - ETA: 0s - loss: 0.4698 - acc: 0.801 - 2s 776us/step - loss: 0.4704 - acc: 0.8040 - val_loss: 0.6674 - val_acc: 0.7533\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.66892 to 0.66741, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 33/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4597 - acc: 0.800 - ETA: 1s - loss: 0.4381 - acc: 0.802 - ETA: 1s - loss: 0.4334 - acc: 0.808 - ETA: 0s - loss: 0.4618 - acc: 0.793 - ETA: 0s - loss: 0.4530 - acc: 0.797 - ETA: 0s - loss: 0.4661 - acc: 0.789 - ETA: 0s - loss: 0.4681 - acc: 0.787 - ETA: 0s - loss: 0.4820 - acc: 0.785 - ETA: 0s - loss: 0.4721 - acc: 0.795 - 2s 766us/step - loss: 0.4683 - acc: 0.8010 - val_loss: 0.6945 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.66741\n",
      "Epoch 34/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4795 - acc: 0.800 - ETA: 1s - loss: 0.4380 - acc: 0.817 - ETA: 1s - loss: 0.4546 - acc: 0.818 - ETA: 0s - loss: 0.4701 - acc: 0.810 - ETA: 0s - loss: 0.4576 - acc: 0.816 - ETA: 0s - loss: 0.4497 - acc: 0.820 - ETA: 0s - loss: 0.4480 - acc: 0.818 - ETA: 0s - loss: 0.4481 - acc: 0.814 - ETA: 0s - loss: 0.4485 - acc: 0.815 - 2s 763us/step - loss: 0.4466 - acc: 0.8150 - val_loss: 0.7220 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.66741\n",
      "Epoch 35/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4579 - acc: 0.810 - ETA: 1s - loss: 0.4432 - acc: 0.815 - ETA: 0s - loss: 0.4371 - acc: 0.813 - ETA: 0s - loss: 0.4272 - acc: 0.817 - ETA: 0s - loss: 0.4272 - acc: 0.819 - ETA: 0s - loss: 0.4356 - acc: 0.818 - ETA: 0s - loss: 0.4338 - acc: 0.821 - ETA: 0s - loss: 0.4411 - acc: 0.820 - ETA: 0s - loss: 0.4402 - acc: 0.820 - 2s 763us/step - loss: 0.4395 - acc: 0.8215 - val_loss: 0.6722 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.66741\n",
      "Epoch 36/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4646 - acc: 0.830 - ETA: 1s - loss: 0.4107 - acc: 0.855 - ETA: 1s - loss: 0.4202 - acc: 0.843 - ETA: 0s - loss: 0.4290 - acc: 0.831 - ETA: 0s - loss: 0.4240 - acc: 0.832 - ETA: 0s - loss: 0.4332 - acc: 0.829 - ETA: 0s - loss: 0.4404 - acc: 0.823 - ETA: 0s - loss: 0.4423 - acc: 0.827 - ETA: 0s - loss: 0.4445 - acc: 0.824 - 2s 838us/step - loss: 0.4456 - acc: 0.8205 - val_loss: 0.6837 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.66741\n",
      "Epoch 37/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3890 - acc: 0.845 - ETA: 1s - loss: 0.4309 - acc: 0.835 - ETA: 1s - loss: 0.4197 - acc: 0.845 - ETA: 1s - loss: 0.4235 - acc: 0.840 - ETA: 0s - loss: 0.4298 - acc: 0.830 - ETA: 0s - loss: 0.4193 - acc: 0.835 - ETA: 0s - loss: 0.4243 - acc: 0.830 - ETA: 0s - loss: 0.4315 - acc: 0.825 - ETA: 0s - loss: 0.4321 - acc: 0.826 - 2s 887us/step - loss: 0.4369 - acc: 0.8230 - val_loss: 0.6960 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.66741\n",
      "Epoch 38/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3823 - acc: 0.850 - ETA: 1s - loss: 0.4598 - acc: 0.815 - ETA: 1s - loss: 0.4685 - acc: 0.801 - ETA: 1s - loss: 0.4597 - acc: 0.806 - ETA: 0s - loss: 0.4710 - acc: 0.802 - ETA: 0s - loss: 0.4791 - acc: 0.796 - ETA: 0s - loss: 0.4704 - acc: 0.802 - ETA: 0s - loss: 0.4583 - acc: 0.808 - ETA: 0s - loss: 0.4574 - acc: 0.811 - 2s 893us/step - loss: 0.4572 - acc: 0.8095 - val_loss: 0.7161 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.66741\n",
      "Epoch 39/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4080 - acc: 0.850 - ETA: 1s - loss: 0.4224 - acc: 0.830 - ETA: 1s - loss: 0.4346 - acc: 0.818 - ETA: 0s - loss: 0.4430 - acc: 0.807 - ETA: 0s - loss: 0.4433 - acc: 0.813 - ETA: 0s - loss: 0.4366 - acc: 0.818 - ETA: 0s - loss: 0.4401 - acc: 0.817 - ETA: 0s - loss: 0.4373 - acc: 0.820 - ETA: 0s - loss: 0.4389 - acc: 0.818 - 2s 803us/step - loss: 0.4350 - acc: 0.8195 - val_loss: 0.6665 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.66741 to 0.66655, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 40/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4978 - acc: 0.785 - ETA: 1s - loss: 0.4680 - acc: 0.792 - ETA: 0s - loss: 0.4513 - acc: 0.808 - ETA: 0s - loss: 0.4437 - acc: 0.816 - ETA: 0s - loss: 0.4433 - acc: 0.818 - ETA: 0s - loss: 0.4335 - acc: 0.819 - ETA: 0s - loss: 0.4437 - acc: 0.812 - ETA: 0s - loss: 0.4326 - acc: 0.816 - ETA: 0s - loss: 0.4268 - acc: 0.816 - 2s 766us/step - loss: 0.4260 - acc: 0.8215 - val_loss: 0.6571 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.66655 to 0.65706, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 41/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4920 - acc: 0.765 - ETA: 1s - loss: 0.4521 - acc: 0.792 - ETA: 1s - loss: 0.4336 - acc: 0.813 - ETA: 0s - loss: 0.4260 - acc: 0.821 - ETA: 0s - loss: 0.4079 - acc: 0.832 - ETA: 0s - loss: 0.4244 - acc: 0.823 - ETA: 0s - loss: 0.4278 - acc: 0.820 - ETA: 0s - loss: 0.4256 - acc: 0.820 - ETA: 0s - loss: 0.4251 - acc: 0.820 - 2s 781us/step - loss: 0.4255 - acc: 0.8215 - val_loss: 0.6706 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.65706\n",
      "Epoch 42/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4184 - acc: 0.820 - ETA: 1s - loss: 0.4097 - acc: 0.827 - ETA: 0s - loss: 0.4018 - acc: 0.836 - ETA: 0s - loss: 0.4158 - acc: 0.835 - ETA: 0s - loss: 0.4175 - acc: 0.832 - ETA: 0s - loss: 0.4199 - acc: 0.830 - ETA: 0s - loss: 0.4288 - acc: 0.822 - ETA: 0s - loss: 0.4239 - acc: 0.823 - ETA: 0s - loss: 0.4233 - acc: 0.830 - 2s 776us/step - loss: 0.4232 - acc: 0.8310 - val_loss: 0.7398 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.65706\n",
      "Epoch 43/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4030 - acc: 0.870 - ETA: 1s - loss: 0.4161 - acc: 0.850 - ETA: 1s - loss: 0.3888 - acc: 0.855 - ETA: 0s - loss: 0.3776 - acc: 0.856 - ETA: 0s - loss: 0.3997 - acc: 0.845 - ETA: 0s - loss: 0.4099 - acc: 0.839 - ETA: 0s - loss: 0.4134 - acc: 0.835 - ETA: 0s - loss: 0.4110 - acc: 0.838 - ETA: 0s - loss: 0.4155 - acc: 0.836 - 2s 775us/step - loss: 0.4183 - acc: 0.8315 - val_loss: 0.7308 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.65706\n",
      "Epoch 44/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3839 - acc: 0.835 - ETA: 1s - loss: 0.4398 - acc: 0.802 - ETA: 1s - loss: 0.4356 - acc: 0.805 - ETA: 0s - loss: 0.4368 - acc: 0.808 - ETA: 0s - loss: 0.4440 - acc: 0.810 - ETA: 0s - loss: 0.4456 - acc: 0.811 - ETA: 0s - loss: 0.4351 - acc: 0.822 - ETA: 0s - loss: 0.4188 - acc: 0.831 - ETA: 0s - loss: 0.4245 - acc: 0.824 - 2s 773us/step - loss: 0.4208 - acc: 0.8250 - val_loss: 0.7012 - val_acc: 0.7000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00044: val_loss did not improve from 0.65706\n",
      "Epoch 45/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4294 - acc: 0.810 - ETA: 1s - loss: 0.4183 - acc: 0.807 - ETA: 1s - loss: 0.3940 - acc: 0.828 - ETA: 0s - loss: 0.4030 - acc: 0.831 - ETA: 0s - loss: 0.4046 - acc: 0.834 - ETA: 0s - loss: 0.4086 - acc: 0.830 - ETA: 0s - loss: 0.4084 - acc: 0.832 - ETA: 0s - loss: 0.4025 - acc: 0.839 - ETA: 0s - loss: 0.3996 - acc: 0.840 - 2s 768us/step - loss: 0.3974 - acc: 0.8445 - val_loss: 0.7115 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.65706\n",
      "Epoch 46/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4059 - acc: 0.825 - ETA: 1s - loss: 0.3868 - acc: 0.840 - ETA: 0s - loss: 0.3806 - acc: 0.845 - ETA: 0s - loss: 0.3909 - acc: 0.842 - ETA: 0s - loss: 0.3854 - acc: 0.847 - ETA: 0s - loss: 0.3881 - acc: 0.843 - ETA: 0s - loss: 0.3911 - acc: 0.836 - ETA: 0s - loss: 0.4002 - acc: 0.829 - ETA: 0s - loss: 0.3916 - acc: 0.833 - 2s 812us/step - loss: 0.3909 - acc: 0.8320 - val_loss: 0.6366 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.65706 to 0.63663, saving model to saved_models/weights.best.my.hdf5\n",
      "Epoch 47/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3807 - acc: 0.835 - ETA: 1s - loss: 0.3757 - acc: 0.847 - ETA: 1s - loss: 0.3760 - acc: 0.851 - ETA: 0s - loss: 0.3799 - acc: 0.848 - ETA: 0s - loss: 0.3812 - acc: 0.846 - ETA: 0s - loss: 0.3784 - acc: 0.845 - ETA: 0s - loss: 0.3833 - acc: 0.840 - ETA: 0s - loss: 0.3829 - acc: 0.841 - ETA: 0s - loss: 0.3823 - acc: 0.843 - 2s 886us/step - loss: 0.3804 - acc: 0.8490 - val_loss: 0.6682 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.63663\n",
      "Epoch 48/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3957 - acc: 0.840 - ETA: 1s - loss: 0.3907 - acc: 0.837 - ETA: 1s - loss: 0.3805 - acc: 0.846 - ETA: 0s - loss: 0.3950 - acc: 0.840 - ETA: 0s - loss: 0.3894 - acc: 0.838 - ETA: 0s - loss: 0.4011 - acc: 0.834 - ETA: 0s - loss: 0.4034 - acc: 0.837 - ETA: 0s - loss: 0.4069 - acc: 0.836 - ETA: 0s - loss: 0.4116 - acc: 0.833 - 2s 872us/step - loss: 0.4111 - acc: 0.8335 - val_loss: 0.7097 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.63663\n",
      "Epoch 49/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4405 - acc: 0.805 - ETA: 1s - loss: 0.4073 - acc: 0.832 - ETA: 1s - loss: 0.3907 - acc: 0.846 - ETA: 0s - loss: 0.3825 - acc: 0.857 - ETA: 0s - loss: 0.3832 - acc: 0.853 - ETA: 0s - loss: 0.3784 - acc: 0.851 - ETA: 0s - loss: 0.3773 - acc: 0.853 - ETA: 0s - loss: 0.3818 - acc: 0.847 - ETA: 0s - loss: 0.3805 - acc: 0.846 - 2s 879us/step - loss: 0.3817 - acc: 0.8450 - val_loss: 0.6536 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.63663\n",
      "Epoch 50/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.4773 - acc: 0.815 - ETA: 1s - loss: 0.4358 - acc: 0.840 - ETA: 1s - loss: 0.4132 - acc: 0.843 - ETA: 1s - loss: 0.4095 - acc: 0.847 - ETA: 0s - loss: 0.3977 - acc: 0.850 - ETA: 0s - loss: 0.3869 - acc: 0.850 - ETA: 0s - loss: 0.3838 - acc: 0.849 - ETA: 0s - loss: 0.3747 - acc: 0.854 - ETA: 0s - loss: 0.3770 - acc: 0.852 - 2s 898us/step - loss: 0.3772 - acc: 0.8510 - val_loss: 0.6811 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.63663\n",
      "Epoch 51/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3835 - acc: 0.840 - ETA: 1s - loss: 0.3687 - acc: 0.857 - ETA: 1s - loss: 0.3759 - acc: 0.865 - ETA: 1s - loss: 0.3861 - acc: 0.856 - ETA: 0s - loss: 0.3867 - acc: 0.857 - ETA: 0s - loss: 0.3810 - acc: 0.858 - ETA: 0s - loss: 0.3721 - acc: 0.863 - ETA: 0s - loss: 0.3722 - acc: 0.860 - ETA: 0s - loss: 0.3726 - acc: 0.858 - 2s 886us/step - loss: 0.3693 - acc: 0.8585 - val_loss: 0.6885 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.63663\n",
      "Epoch 52/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3437 - acc: 0.850 - ETA: 1s - loss: 0.3685 - acc: 0.840 - ETA: 1s - loss: 0.3614 - acc: 0.845 - ETA: 0s - loss: 0.3603 - acc: 0.841 - ETA: 0s - loss: 0.3597 - acc: 0.848 - ETA: 0s - loss: 0.3591 - acc: 0.852 - ETA: 0s - loss: 0.3604 - acc: 0.854 - ETA: 0s - loss: 0.3631 - acc: 0.853 - ETA: 0s - loss: 0.3615 - acc: 0.854 - 2s 866us/step - loss: 0.3673 - acc: 0.8510 - val_loss: 0.7223 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.63663\n",
      "Epoch 53/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3633 - acc: 0.865 - ETA: 1s - loss: 0.3771 - acc: 0.840 - ETA: 1s - loss: 0.3785 - acc: 0.848 - ETA: 1s - loss: 0.3807 - acc: 0.853 - ETA: 0s - loss: 0.3791 - acc: 0.857 - ETA: 0s - loss: 0.3797 - acc: 0.855 - ETA: 0s - loss: 0.3794 - acc: 0.855 - ETA: 0s - loss: 0.3803 - acc: 0.855 - ETA: 0s - loss: 0.3717 - acc: 0.857 - 2s 887us/step - loss: 0.3808 - acc: 0.8530 - val_loss: 0.7708 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.63663\n",
      "Epoch 54/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3086 - acc: 0.895 - ETA: 1s - loss: 0.3425 - acc: 0.880 - ETA: 1s - loss: 0.3617 - acc: 0.870 - ETA: 0s - loss: 0.3652 - acc: 0.865 - ETA: 0s - loss: 0.3641 - acc: 0.861 - ETA: 0s - loss: 0.3594 - acc: 0.862 - ETA: 0s - loss: 0.3534 - acc: 0.860 - ETA: 0s - loss: 0.3569 - acc: 0.857 - ETA: 0s - loss: 0.3571 - acc: 0.860 - 2s 790us/step - loss: 0.3593 - acc: 0.8580 - val_loss: 0.7350 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.63663\n",
      "Epoch 55/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3793 - acc: 0.850 - ETA: 1s - loss: 0.3634 - acc: 0.847 - ETA: 0s - loss: 0.3579 - acc: 0.848 - ETA: 0s - loss: 0.3370 - acc: 0.860 - ETA: 0s - loss: 0.3432 - acc: 0.860 - ETA: 0s - loss: 0.3459 - acc: 0.860 - ETA: 0s - loss: 0.3415 - acc: 0.865 - ETA: 0s - loss: 0.3450 - acc: 0.866 - ETA: 0s - loss: 0.3481 - acc: 0.863 - 2s 767us/step - loss: 0.3497 - acc: 0.8650 - val_loss: 0.6761 - val_acc: 0.7533\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.63663\n",
      "Epoch 56/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3368 - acc: 0.880 - ETA: 1s - loss: 0.3855 - acc: 0.865 - ETA: 0s - loss: 0.3785 - acc: 0.855 - ETA: 0s - loss: 0.3706 - acc: 0.858 - ETA: 0s - loss: 0.3577 - acc: 0.863 - ETA: 0s - loss: 0.3600 - acc: 0.861 - ETA: 0s - loss: 0.3541 - acc: 0.863 - ETA: 0s - loss: 0.3521 - acc: 0.862 - ETA: 0s - loss: 0.3528 - acc: 0.861 - 2s 760us/step - loss: 0.3480 - acc: 0.8600 - val_loss: 0.6975 - val_acc: 0.7467\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.63663\n",
      "Epoch 57/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3773 - acc: 0.845 - ETA: 1s - loss: 0.3761 - acc: 0.837 - ETA: 0s - loss: 0.3836 - acc: 0.835 - ETA: 0s - loss: 0.3857 - acc: 0.838 - ETA: 0s - loss: 0.3824 - acc: 0.835 - ETA: 0s - loss: 0.3774 - acc: 0.840 - ETA: 0s - loss: 0.3666 - acc: 0.851 - ETA: 0s - loss: 0.3553 - acc: 0.858 - ETA: 0s - loss: 0.3494 - acc: 0.863 - 2s 761us/step - loss: 0.3442 - acc: 0.8685 - val_loss: 0.7112 - val_acc: 0.7467\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.63663\n",
      "Epoch 58/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3339 - acc: 0.890 - ETA: 1s - loss: 0.3311 - acc: 0.880 - ETA: 0s - loss: 0.3424 - acc: 0.865 - ETA: 0s - loss: 0.3422 - acc: 0.868 - ETA: 0s - loss: 0.3417 - acc: 0.866 - ETA: 0s - loss: 0.3430 - acc: 0.863 - ETA: 0s - loss: 0.3326 - acc: 0.867 - ETA: 0s - loss: 0.3351 - acc: 0.866 - ETA: 0s - loss: 0.3365 - acc: 0.868 - 2s 758us/step - loss: 0.3343 - acc: 0.8700 - val_loss: 0.6690 - val_acc: 0.7533\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.63663\n",
      "Epoch 59/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3529 - acc: 0.835 - ETA: 1s - loss: 0.3445 - acc: 0.847 - ETA: 1s - loss: 0.3256 - acc: 0.863 - ETA: 0s - loss: 0.3215 - acc: 0.870 - ETA: 0s - loss: 0.3272 - acc: 0.865 - ETA: 0s - loss: 0.3231 - acc: 0.866 - ETA: 0s - loss: 0.3252 - acc: 0.867 - ETA: 0s - loss: 0.3306 - acc: 0.865 - ETA: 0s - loss: 0.3301 - acc: 0.866 - 2s 755us/step - loss: 0.3341 - acc: 0.8640 - val_loss: 0.7210 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.63663\n",
      "Epoch 60/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3398 - acc: 0.850 - ETA: 1s - loss: 0.3202 - acc: 0.870 - ETA: 0s - loss: 0.3231 - acc: 0.876 - ETA: 0s - loss: 0.3330 - acc: 0.867 - ETA: 0s - loss: 0.3345 - acc: 0.869 - ETA: 0s - loss: 0.3304 - acc: 0.872 - ETA: 0s - loss: 0.3254 - acc: 0.876 - ETA: 0s - loss: 0.3271 - acc: 0.876 - ETA: 0s - loss: 0.3221 - acc: 0.881 - 2s 753us/step - loss: 0.3253 - acc: 0.8780 - val_loss: 0.7571 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.63663\n",
      "Epoch 61/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3484 - acc: 0.855 - ETA: 1s - loss: 0.2999 - acc: 0.870 - ETA: 0s - loss: 0.2886 - acc: 0.885 - ETA: 0s - loss: 0.3004 - acc: 0.883 - ETA: 0s - loss: 0.3158 - acc: 0.877 - ETA: 0s - loss: 0.3233 - acc: 0.875 - ETA: 0s - loss: 0.3292 - acc: 0.872 - ETA: 0s - loss: 0.3279 - acc: 0.875 - ETA: 0s - loss: 0.3291 - acc: 0.875 - 2s 758us/step - loss: 0.3264 - acc: 0.8755 - val_loss: 0.7871 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.63663\n",
      "Epoch 62/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.2772 - acc: 0.885 - ETA: 1s - loss: 0.3098 - acc: 0.870 - ETA: 1s - loss: 0.3101 - acc: 0.873 - ETA: 0s - loss: 0.3074 - acc: 0.881 - ETA: 0s - loss: 0.3169 - acc: 0.879 - ETA: 0s - loss: 0.3195 - acc: 0.879 - ETA: 0s - loss: 0.3168 - acc: 0.882 - ETA: 0s - loss: 0.3207 - acc: 0.876 - ETA: 0s - loss: 0.3276 - acc: 0.871 - 2s 754us/step - loss: 0.3303 - acc: 0.8695 - val_loss: 0.6639 - val_acc: 0.7333\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.63663\n",
      "Epoch 63/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.2688 - acc: 0.890 - ETA: 1s - loss: 0.3148 - acc: 0.877 - ETA: 0s - loss: 0.3145 - acc: 0.885 - ETA: 0s - loss: 0.3144 - acc: 0.880 - ETA: 0s - loss: 0.3143 - acc: 0.879 - ETA: 0s - loss: 0.3226 - acc: 0.874 - ETA: 0s - loss: 0.3133 - acc: 0.882 - ETA: 0s - loss: 0.3059 - acc: 0.888 - ETA: 0s - loss: 0.3013 - acc: 0.889 - 1s 749us/step - loss: 0.3067 - acc: 0.8865 - val_loss: 0.7489 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.63663\n",
      "Epoch 64/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3231 - acc: 0.855 - ETA: 1s - loss: 0.3398 - acc: 0.867 - ETA: 1s - loss: 0.3494 - acc: 0.866 - ETA: 0s - loss: 0.3321 - acc: 0.872 - ETA: 0s - loss: 0.3298 - acc: 0.873 - ETA: 0s - loss: 0.3268 - acc: 0.874 - ETA: 0s - loss: 0.3296 - acc: 0.871 - ETA: 0s - loss: 0.3249 - acc: 0.873 - ETA: 0s - loss: 0.3294 - acc: 0.872 - 2s 755us/step - loss: 0.3258 - acc: 0.8740 - val_loss: 0.7080 - val_acc: 0.7533\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.63663\n",
      "Epoch 65/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3611 - acc: 0.865 - ETA: 1s - loss: 0.3073 - acc: 0.890 - ETA: 0s - loss: 0.2975 - acc: 0.893 - ETA: 0s - loss: 0.2908 - acc: 0.900 - ETA: 0s - loss: 0.2777 - acc: 0.903 - ETA: 0s - loss: 0.2820 - acc: 0.896 - ETA: 0s - loss: 0.2953 - acc: 0.890 - ETA: 0s - loss: 0.2946 - acc: 0.890 - ETA: 0s - loss: 0.3032 - acc: 0.886 - 2s 751us/step - loss: 0.3093 - acc: 0.8820 - val_loss: 0.6945 - val_acc: 0.7467\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.63663\n",
      "Epoch 66/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3178 - acc: 0.860 - ETA: 1s - loss: 0.3160 - acc: 0.857 - ETA: 0s - loss: 0.3075 - acc: 0.871 - ETA: 0s - loss: 0.3185 - acc: 0.871 - ETA: 0s - loss: 0.3164 - acc: 0.876 - ETA: 0s - loss: 0.3125 - acc: 0.877 - ETA: 0s - loss: 0.3066 - acc: 0.884 - ETA: 0s - loss: 0.3111 - acc: 0.880 - ETA: 0s - loss: 0.3054 - acc: 0.882 - 1s 749us/step - loss: 0.3045 - acc: 0.8810 - val_loss: 0.7374 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.63663\n",
      "Epoch 67/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.2358 - acc: 0.920 - ETA: 1s - loss: 0.2534 - acc: 0.907 - ETA: 0s - loss: 0.2600 - acc: 0.901 - ETA: 0s - loss: 0.2664 - acc: 0.891 - ETA: 0s - loss: 0.2707 - acc: 0.893 - ETA: 0s - loss: 0.2737 - acc: 0.888 - ETA: 0s - loss: 0.2779 - acc: 0.885 - ETA: 0s - loss: 0.2750 - acc: 0.887 - ETA: 0s - loss: 0.2764 - acc: 0.888 - 2s 758us/step - loss: 0.2830 - acc: 0.8865 - val_loss: 0.7746 - val_acc: 0.7067\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.63663\n",
      "Epoch 68/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.3377 - acc: 0.840 - ETA: 1s - loss: 0.2975 - acc: 0.865 - ETA: 1s - loss: 0.3020 - acc: 0.866 - ETA: 0s - loss: 0.3124 - acc: 0.862 - ETA: 0s - loss: 0.3133 - acc: 0.871 - ETA: 0s - loss: 0.3088 - acc: 0.875 - ETA: 0s - loss: 0.3138 - acc: 0.872 - ETA: 0s - loss: 0.3146 - acc: 0.870 - ETA: 0s - loss: 0.3039 - acc: 0.875 - 2s 806us/step - loss: 0.3023 - acc: 0.8775 - val_loss: 0.6710 - val_acc: 0.7467\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.63663\n",
      "Epoch 69/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.2819 - acc: 0.925 - ETA: 1s - loss: 0.2738 - acc: 0.917 - ETA: 1s - loss: 0.2634 - acc: 0.915 - ETA: 0s - loss: 0.2721 - acc: 0.906 - ETA: 0s - loss: 0.2884 - acc: 0.899 - ETA: 0s - loss: 0.2817 - acc: 0.899 - ETA: 0s - loss: 0.2917 - acc: 0.894 - ETA: 0s - loss: 0.2946 - acc: 0.892 - ETA: 0s - loss: 0.2953 - acc: 0.892 - 2s 808us/step - loss: 0.2925 - acc: 0.8910 - val_loss: 0.7138 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.63663\n",
      "Epoch 70/70\n",
      "2000/2000 [==============================] - ETA: 1s - loss: 0.2865 - acc: 0.890 - ETA: 1s - loss: 0.2793 - acc: 0.880 - ETA: 1s - loss: 0.2801 - acc: 0.881 - ETA: 0s - loss: 0.2688 - acc: 0.890 - ETA: 0s - loss: 0.2824 - acc: 0.879 - ETA: 0s - loss: 0.2857 - acc: 0.876 - ETA: 0s - loss: 0.2826 - acc: 0.878 - ETA: 0s - loss: 0.2843 - acc: 0.878 - ETA: 0s - loss: 0.2866 - acc: 0.877 - 2s 810us/step - loss: 0.2912 - acc: 0.8760 - val_loss: 0.6660 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.63663\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "checkpoint_filepath = 'saved_models/weights.best.my.hdf5'\n",
    "\n",
    "my_checkpointer = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "my_model.fit(train_data, train_targets, \n",
    "          validation_data=(valid_data, valid_targets),\n",
    "          epochs=70, batch_size=200, callbacks=[my_checkpointer], verbose=1)\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.74688307414986\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "my_model.load_weights(checkpoint_filepath)\n",
    "\n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluavate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.79439494532085\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "my_predictions = [my_model.predict(np.expand_dims(feature, axis=0)) for feature in test_data]\n",
    "\n",
    "# test_accuracy = 100 * np.sum(np.array(my_predictions)==np.argmax(test_targets, axis=1)) / len(my_predictions)\n",
    "# print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "\n",
    "with open('my_transfer.csv', 'w', newline='') as csvfile:\n",
    "    result_writger = csv.writer(csvfile)\n",
    "    result_writger.writerow(['Id', 'task_1', 'task_2'])\n",
    "    for test_filepath, test_prediction in zip(test_files, my_predictions):\n",
    "        result_writger.writerow([test_filepath, test_prediction[0][0], test_prediction[0][2]])\n",
    "        \n",
    "print_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category 1 Score: 0.526\n",
    "Category 2 Score: 0.606\n",
    "Category 3 Score: 0.566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ROC Curves](images/figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without transfer learning, loading images into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10808795256416488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [07:45<00:00,  4.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [01:01<00:00,  2.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [06:12<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 384, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "train_tensors = paths_to_tensor(tqdm(train_files))\n",
    "train_tensors = train_tensors.astype('float32') / 255\n",
    "\n",
    "valid_tensors = paths_to_tensor(tqdm(valid_files))\n",
    "valid_tensors = valid_tensors.astype('float32') / 255\n",
    "\n",
    "test_tensors = paths_to_tensor(tqdm(test_files))\n",
    "test_tensors = test_tensors.astype('float32') / 255\n",
    "\n",
    "print(train_tensors.shape)\n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.867575357594311\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_411 (Conv2D)          (None, 384, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 192, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 192, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_412 (Conv2D)          (None, 192, 128, 64)      9280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 96, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 96, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_413 (Conv2D)          (None, 96, 64, 256)       147712    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 48, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 48, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_414 (Conv2D)          (None, 48, 32, 1024)      2360320   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 24, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 24, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_415 (Conv2D)          (None, 24, 16, 2048)      18876416  \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 12, 8, 2048)       0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12, 8, 2048)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 21,400,323\n",
      "Trainable params: 21,400,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "from keras.layers import Conv2D, Dropout, Flatten, Dense, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "my_model = Sequential()\n",
    "\n",
    "my_model.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', \n",
    "                        input_shape=train_tensors.shape[1:]))\n",
    "my_model.add(MaxPooling2D(pool_size=2))\n",
    "my_model.add(Dropout(0.2))\n",
    "\n",
    "my_model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "my_model.add(MaxPooling2D(pool_size=2))\n",
    "my_model.add(Dropout(0.2))\n",
    "\n",
    "my_model.add(Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'))\n",
    "my_model.add(MaxPooling2D(pool_size=2))\n",
    "my_model.add(Dropout(0.2))\n",
    "\n",
    "my_model.add(Conv2D(filters=1024, kernel_size=3, padding='same', activation='relu'))\n",
    "my_model.add(MaxPooling2D(pool_size=2))\n",
    "my_model.add(Dropout(0.1))\n",
    "\n",
    "my_model.add(Conv2D(filters=2048, kernel_size=3, padding='same', activation='relu'))\n",
    "my_model.add(MaxPooling2D(pool_size=2))\n",
    "my_model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "my_model.add(GlobalAveragePooling2D())\n",
    "\n",
    "my_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "my_model.summary()\n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.94980908334231\n"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.022846783856888\n",
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/40\n",
      "1400/2000 [====================>.........] - ETA: 31:36 - loss: 1.1120 - acc: 0.17 - ETA: 27:51 - loss: 3.1349 - acc: 0.42 - ETA: 25:58 - loss: 4.1315 - acc: 0.49 - ETA: 23:48 - loss: 4.3881 - acc: 0.53 - ETA: 21:55 - loss: 4.4131 - acc: 0.57 - ETA: 20:14 - loss: 4.4835 - acc: 0.59 - ETA: 18:39 - loss: 4.5798 - acc: 0.60 - ETA: 17:08 - loss: 4.6722 - acc: 0.61 - ETA: 15:38 - loss: 4.8336 - acc: 0.61 - ETA: 14:12 - loss: 4.7854 - acc: 0.62 - ETA: 12:47 - loss: 4.8779 - acc: 0.62 - ETA: 11:20 - loss: 4.8475 - acc: 0.63 - ETA: 9:55 - loss: 4.7846 - acc: 0.6446 - ETA: 8:29 - loss: 4.7997 - acc: 0.6479"
     ]
    }
   ],
   "source": [
    "start_timer()\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "\n",
    "checkpoint_filepath = 'saved_models/weights.best.my.hdf5'\n",
    "\n",
    "my_checkpointer = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "my_model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=40, batch_size=100, callbacks=[my_checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timer()\n",
    "\n",
    "my_model.load_weights(checkpoint_filepath)\n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timer()\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "my_predictions = [my_model.predict(np.expand_dims(feature, axis=0)) for feature in test_tensors]\n",
    "\n",
    "# test_accuracy = 100 * np.sum(np.array(my_predictions)==np.argmax(test_targets, axis=1)) / len(my_predictions)\n",
    "# print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "\n",
    "with open('my_cnn.csv', 'w', newline='') as csvfile:\n",
    "    result_writger = csv.writer(csvfile)\n",
    "    result_writger.writerow(['Id', 'task_1', 'task_2'])\n",
    "    for test_filepath, test_prediction in zip(test_files, my_predictions):\n",
    "        result_writger.writerow([test_filepath, test_prediction[0][0], test_prediction[0][2]])\n",
    "        \n",
    "\n",
    "print_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
